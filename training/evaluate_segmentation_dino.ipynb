{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f4114a",
   "metadata": {},
   "source": [
    "# DinoV2-Based Semantic Segmentation Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook implements an evaluation framework for semantic segmentation using Facebook's DinoV2 architecture. The model is specifically adapted for peatland environment analysis, providing detailed scene understanding for autonomous navigation.\n",
    "\n",
    "## Semantic Classes\n",
    "\n",
    "### Navigation-Critical Classes\n",
    "1. **Paths**\n",
    "   - Primary navigation routes\n",
    "   - Constructed walkways and trails\n",
    "   - Maintained access routes\n",
    "\n",
    "2. **Natural Ground**\n",
    "   - Soil and bare earth\n",
    "   - Traversable natural surfaces\n",
    "   - Potential alternative routes\n",
    "\n",
    "### Environmental Features\n",
    "3. **Trees**\n",
    "   - Individual trees and trunks\n",
    "   - Root systems\n",
    "   - Canopy elements\n",
    "\n",
    "4. **Vegetation**\n",
    "   - Low-lying vegetation\n",
    "   - Bushes and shrubs\n",
    "   - Ground cover\n",
    "\n",
    "5. **Ignore**\n",
    "   - Regions not relevant for navigation\n",
    "   - Undefined or ambiguous areas\n",
    "   - Areas outside annotation scope\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Model Architecture\n",
    "- DinoV2 backbone for robust feature extraction\n",
    "- Self-supervised pre-training advantages\n",
    "- Custom decoder for semantic segmentation\n",
    "- Multi-scale feature processing\n",
    "\n",
    "### Evaluation Components\n",
    "1. **Quantitative Assessment**\n",
    "   - Class-wise IoU metrics\n",
    "   - Pixel accuracy measurements\n",
    "   - Precision-recall analysis\n",
    "\n",
    "2. **Qualitative Analysis**\n",
    "   - Visual prediction comparison\n",
    "   - Error pattern identification\n",
    "   - Edge case examination\n",
    "\n",
    "3. **Performance Documentation**\n",
    "   - Comprehensive metric logging\n",
    "   - Statistical analysis\n",
    "   - Visual result compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14e1a7",
   "metadata": {},
   "source": [
    "## 1. Required Libraries\n",
    "\n",
    "Key dependencies for the evaluation:\n",
    "\n",
    "- **torch**: Deep learning framework\n",
    "- **segmentation_models_pytorch**: Semantic segmentation utilities\n",
    "- **transformers**: Access to DinoV2 model and processor\n",
    "- **albumentations**: Image augmentation and transformation\n",
    "- **torchmetrics**: Evaluation metrics (JaccardIndex)\n",
    "- **PIL, numpy, pandas**: Data handling and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e6240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "from transformers import Dinov2Model, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcffe3",
   "metadata": {},
   "source": [
    "## 2. Dataset and Model Architecture\n",
    "\n",
    "### Dataset Implementation\n",
    "\n",
    "The `PeatlandDinoDataset` class implements a specialized data pipeline for DinoV2-based segmentation:\n",
    "\n",
    "1. **Data Loading**\n",
    "   - Custom PyTorch Dataset implementation\n",
    "   - Synchronized image-mask pair handling\n",
    "   - Efficient memory management\n",
    "   - Dynamic file path resolution\n",
    "\n",
    "2. **Image Processing**\n",
    "   - DinoV2-specific preprocessing\n",
    "   - Automatic image format conversion\n",
    "   - Resolution standardization\n",
    "   - Consistent normalization\n",
    "\n",
    "3. **Mask Handling**\n",
    "   - Multi-class label encoding\n",
    "   - Spatial alignment maintenance\n",
    "   - Memory-efficient storage\n",
    "   - Label verification\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The `DinoV2ForSemanticSegmentation` implements a custom architecture combining DinoV2's powerful feature extraction with semantic segmentation capabilities:\n",
    "\n",
    "1. **Backbone Network**\n",
    "   - Pretrained DinoV2-base encoder\n",
    "   - Frozen weights for stable features\n",
    "   - Multi-scale feature extraction\n",
    "   - Self-supervised learning benefits\n",
    "\n",
    "2. **Decoder Design**\n",
    "   - Progressive upsampling pathway\n",
    "   - Feature refinement blocks\n",
    "   - Skip connections for detail preservation\n",
    "   - Multi-scale feature fusion\n",
    "\n",
    "3. **Output Generation**\n",
    "   - 5-class probability maps\n",
    "   - High-resolution predictions\n",
    "   - Spatial consistency preservation\n",
    "   - Efficient inference pipeline\n",
    "\n",
    "This architecture leverages DinoV2's robust feature extraction while adapting it specifically for the semantic segmentation task in peatland environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5630cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef9c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeatlandDinoDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset adapted for DinoV2's image processor.\"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, image_processor, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=mask)\n",
    "            image = Image.fromarray(augmented['image'])\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return pixel_values, mask\n",
    "\n",
    "class DinoV2ForSemanticSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(DinoV2ForSemanticSegmentation, self).__init__()\n",
    "        self.dinov2 = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.dinov2(pixel_values, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        patch_tokens = last_hidden_state[:, 1:, :]\n",
    "        batch_size, seq_len, num_channels = patch_tokens.shape\n",
    "        height = width = int(seq_len**0.5)\n",
    "        feature_map = patch_tokens.permute(0, 2, 1).contiguous().reshape(batch_size, num_channels, height, width)\n",
    "        logits = self.head(feature_map)\n",
    "        final_logits = nn.functional.interpolate(logits, size=(DINO_IMAGE_SIZE, DINO_IMAGE_SIZE), mode='bilinear', align_corners=False)\n",
    "        return final_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de344",
   "metadata": {},
   "source": [
    "## 3. Data Processing Pipeline\n",
    "\n",
    "The evaluation pipeline implements a sophisticated data processing workflow optimized for DinoV2-based segmentation:\n",
    "\n",
    "### 1. DinoV2 Image Processor\n",
    "\n",
    "The image processor handles DinoV2-specific requirements:\n",
    "\n",
    "1. **Input Formatting**\n",
    "   - Resolution standardization (224x224)\n",
    "   - Channel normalization\n",
    "   - Tensor format conversion\n",
    "   - Batch dimension handling\n",
    "\n",
    "2. **Preprocessing Steps**\n",
    "   - Color space normalization\n",
    "   - Aspect ratio preservation\n",
    "   - Pixel value scaling\n",
    "   - Format compatibility checks\n",
    "\n",
    "3. **Quality Assurance**\n",
    "   - Input validation\n",
    "   - Format verification\n",
    "   - Dimension checking\n",
    "   - Error handling\n",
    "\n",
    "### 2. Evaluation Transforms\n",
    "\n",
    "The transformation pipeline ensures consistent data processing:\n",
    "\n",
    "1. **Spatial Processing**\n",
    "   - Fixed-size resizing (224x224)\n",
    "   - Aspect ratio management\n",
    "   - Coordinate system preservation\n",
    "   - Border handling\n",
    "\n",
    "2. **Mask Processing**\n",
    "   - Label value preservation\n",
    "   - Interpolation method selection\n",
    "   - Class boundary preservation\n",
    "   - Spatial alignment maintenance\n",
    "\n",
    "3. **Quality Controls**\n",
    "   - Transform validation\n",
    "   - Label integrity checks\n",
    "   - Spatial consistency verification\n",
    "   - Error logging\n",
    "\n",
    "This comprehensive processing ensures reliable and consistent evaluation across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c7b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "eval_transform = A.Compose([A.Resize(height=DINO_IMAGE_SIZE, width=DINO_IMAGE_SIZE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97766aa1",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from run: dinov2_2025-08-03_17-48-54\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Specify the training run to evaluate\n",
    "RUN_NAME = \"dinov2_2025-08-03_17-48-54\"  # Update this for each evaluation\n",
    "\n",
    "# Set up paths and directories\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME                 # Directory for storing results\n",
    "MODEL_PATH = METRICS_DIR / \"best_model.pth\"             # Path to saved model weights\n",
    "BATCH_SIZE = 4                                          # Evaluation batch size\n",
    "\n",
    "# Determine compute device\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"           # Use GPU if available\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"  # Use Apple Silicon if available\n",
    "else: DEVICE = \"cpu\"                                    # Fall back to CPU\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Evaluating model from run: {RUN_NAME}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9666182",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426eb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 204 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "BASE_PROCESSED_DIR = Path(\"../data/processed/segmentation\")\n",
    "TEST_IMG_DIR = BASE_PROCESSED_DIR / \"test\" / \"images\"\n",
    "TEST_MASK_DIR = BASE_PROCESSED_DIR / \"test\" / \"masks\"\n",
    "\n",
    "test_dataset = PeatlandDinoDataset(\n",
    "    images_dir=TEST_IMG_DIR,\n",
    "    masks_dir=TEST_MASK_DIR,\n",
    "    image_processor=processor,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Loaded {len(test_dataset)} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8241d",
   "metadata": {},
   "source": [
    "## 6. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b16e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = DinoV2ForSemanticSegmentation(num_classes=5).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deae95",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation Methodology\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "1. **Intersection over Union (IoU)**\n",
    "   \n",
    "   **Per-Class Analysis:**\n",
    "   - **PATH**: Critical for navigation planning\n",
    "   - **NATURAL_GROUND**: Alternative route assessment\n",
    "   - **TREE**: Obstacle avoidance\n",
    "   - **VEGETATION**: Terrain characterization\n",
    "   - **IGNORE**: Evaluation exclusion zones\n",
    "\n",
    "   **Calculation Process:**\n",
    "   - Pixel-wise intersection computation\n",
    "   - Union area determination\n",
    "   - Class-specific normalization\n",
    "   - Statistical aggregation\n",
    "\n",
    "2. **Mean IoU (mIoU)**\n",
    "   \n",
    "   **Characteristics:**\n",
    "   - Weighted average across classes\n",
    "   - Balanced performance indicator\n",
    "   - Global model assessment\n",
    "   - Cross-dataset comparison metric\n",
    "\n",
    "3. **Pixel Accuracy**\n",
    "   \n",
    "   **Features:**\n",
    "   - Raw classification success rate\n",
    "   - Class-agnostic performance\n",
    "   - Intuitive interpretation\n",
    "   - Baseline metric validation\n",
    "\n",
    "### Class Significance\n",
    "\n",
    "1. **Navigation Classes**\n",
    "   - **PATH**: Primary navigation routes\n",
    "     * Critical for path planning\n",
    "     * Safety-critical segmentation\n",
    "     * Route optimization input\n",
    "   \n",
    "   - **NATURAL_GROUND**: Secondary routes\n",
    "     * Alternative path options\n",
    "     * Emergency route planning\n",
    "     * Terrain accessibility assessment\n",
    "\n",
    "2. **Obstacle Classes**\n",
    "   - **TREE**: Primary obstacles\n",
    "     * Collision avoidance\n",
    "     * Navigation boundaries\n",
    "     * Landmark identification\n",
    "   \n",
    "   - **VEGETATION**: Environmental context\n",
    "     * Terrain characterization\n",
    "     * Seasonal variability\n",
    "     * Navigation complexity\n",
    "\n",
    "3. **Special Handling**\n",
    "   - **IGNORE**: Excluded regions\n",
    "     * Evaluation boundary definition\n",
    "     * Uncertainty handling\n",
    "     * Data quality control\n",
    "\n",
    "This comprehensive evaluation framework ensures thorough assessment of the model's performance in the context of autonomous navigation requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04300b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████| 51/51 [00:20<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Pixel Accuracy: 86.29%\n",
      "Mean IoU (mIoU): 0.6565\n",
      "\n",
      "IoU per Class:\n",
      "  - PATH: 0.7115\n",
      "  - NATURAL_GROUND: 0.7868\n",
      "  - TREE: 0.2320\n",
      "  - VEGETATION: 0.7450\n",
      "  - IGNORE: 0.8070\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "CLASS_NAMES = [\"PATH\", \"NATURAL_GROUND\", \"TREE\", \"VEGETATION\", \"IGNORE\"]\n",
    "jaccard = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(DEVICE)\n",
    "total_correct_pixels, total_pixels = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        jaccard.update(preds, masks)\n",
    "        total_correct_pixels += (preds == masks).sum().item()\n",
    "        total_pixels += torch.numel(masks)\n",
    "\n",
    "pixel_accuracy = (total_correct_pixels / total_pixels) * 100\n",
    "iou_per_class = jaccard.compute()\n",
    "mean_iou = iou_per_class.mean()\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Overall Pixel Accuracy: {pixel_accuracy:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {mean_iou:.4f}\")\n",
    "print(\"\\nIoU per Class:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  - {class_name}: {iou_per_class[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22583c",
   "metadata": {},
   "source": [
    "## 8. Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b798271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics saved to: metrics/dinov2_2025-08-03_17-48-54/test_set_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "    'Metric': ['Pixel Accuracy', 'Mean IoU'] + [f'IoU_{name}' for name in CLASS_NAMES],\n",
    "    'Value': [pixel_accuracy, mean_iou.item()] + iou_per_class.cpu().numpy().tolist()\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "output_csv_path = METRICS_DIR / \"test_set_evaluation.csv\"\n",
    "metrics_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nEvaluation metrics saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9bb16",
   "metadata": {},
   "source": [
    "## 9. Qualitative Analysis and Visualization\n",
    "\n",
    "### Visualization Components\n",
    "\n",
    "1. **Original Image Display**\n",
    "   - Full resolution presentation\n",
    "   - Natural color reproduction\n",
    "   - Detail preservation\n",
    "   - Context maintenance\n",
    "   \n",
    "2. **Ground Truth Visualization**\n",
    "   \n",
    "   **Color Coding Schema:**\n",
    "   - **PATH**: Purple (60, 16, 152)\n",
    "     * High visibility against natural backgrounds\n",
    "     * Clear path delineation\n",
    "     * Navigation focus\n",
    "   \n",
    "   - **NATURAL_GROUND**: Blue (132, 41, 246)\n",
    "     * Distinct from path regions\n",
    "     * Clear terrain boundary identification\n",
    "     * Alternative route highlighting\n",
    "   \n",
    "   - **TREE**: Light Blue (110, 193, 228)\n",
    "     * Obstacle emphasis\n",
    "     * Clear separation from vegetation\n",
    "     * Structural element identification\n",
    "   \n",
    "   - **VEGETATION**: Yellow (254, 221, 58)\n",
    "     * High contrast with other classes\n",
    "     * Clear vegetation boundary visualization\n",
    "     * Seasonal variation representation\n",
    "   \n",
    "   - **IGNORE**: Black (0, 0, 0)\n",
    "     * Clear exclusion zone marking\n",
    "     * Evaluation boundary indication\n",
    "     * Data quality visualization\n",
    "\n",
    "3. **Prediction Visualization**\n",
    "   - Identical color scheme to ground truth\n",
    "   - Direct visual comparison capability\n",
    "   - Error pattern identification\n",
    "   - Quality assessment support\n",
    "\n",
    "### Analysis Capabilities\n",
    "\n",
    "1. **Comparative Assessment**\n",
    "   - Side-by-side comparison\n",
    "   - Error pattern identification\n",
    "   - Boundary accuracy analysis\n",
    "   - Consistency verification\n",
    "\n",
    "2. **Quality Control**\n",
    "   - Resolution consistency check\n",
    "   - Color reproduction validation\n",
    "   - Label integrity verification\n",
    "   - Visual artifact detection\n",
    "\n",
    "3. **Documentation**\n",
    "   - High-resolution PNG output\n",
    "   - Standardized naming convention\n",
    "   - Organized directory structure\n",
    "   - Batch processing support\n",
    "\n",
    "The visualization system provides comprehensive qualitative analysis capabilities, complementing the quantitative metrics for thorough model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54ecd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 visualization samples to: metrics/dinov2_2025-08-03_17-48-54/visualizations\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset, model, device, num_samples=5):\n",
    "    vis_dir = METRICS_DIR / \"visualizations\"\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    color_map = np.array([\n",
    "        [60, 16, 152], [132, 41, 246], [110, 193, 228],\n",
    "        [254, 221, 58], [0, 0, 0]\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get processed data for the model\n",
    "            image_tensor, gt_mask = dataset[i]\n",
    "            \n",
    "            # Load the original, unprocessed image for clean visualization\n",
    "            original_image_path = dataset.images_dir / dataset.image_filenames[i]\n",
    "            display_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Get prediction\n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Apply color map to masks\n",
    "            gt_mask_color = color_map[gt_mask.cpu().numpy()]\n",
    "            pred_mask_color = color_map[pred_mask]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "            fig.suptitle(f\"Sample {i}\", fontsize=16)\n",
    "            \n",
    "            ax[0].imshow(display_image)\n",
    "            ax[0].set_title(\"Original Image\")\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(gt_mask_color)\n",
    "            ax[1].set_title(\"Ground Truth Mask\")\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            ax[2].imshow(pred_mask_color)\n",
    "            ax[2].set_title(\"Model Prediction\")\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Saved {num_samples} visualization samples to: {vis_dir}\")\n",
    "\n",
    "visualize_predictions(test_dataset, model, DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
