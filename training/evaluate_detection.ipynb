{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfb2f96",
   "metadata": {},
   "source": [
    "# YOLO Object Detection Evaluation for Peatland Navigation\n",
    "\n",
    "## Overview\n",
    "This notebook implements an evaluation framework for the YOLO-based object detection model trained for peatland navigation. The evaluation systematically assesses the model's ability to detect and localize key navigational elements in peatland environments.\n",
    "\n",
    "## Evaluation Framework\n",
    "\n",
    "### 1. Detection Performance Metrics\n",
    "- **Mean Average Precision (mAP)**\n",
    "  - Multiple IoU thresholds (0.50-0.95)\n",
    "  - Per-class performance analysis\n",
    "  - Precision-Recall characteristics\n",
    "\n",
    "### 2. Testing Methodology\n",
    "- Dedicated test set validation\n",
    "- Confidence threshold analysis\n",
    "- Non-Maximum Suppression assessment\n",
    "- Real-world scenario testing\n",
    "\n",
    "### 3. Analysis Components\n",
    "- Statistical performance metrics\n",
    "- Visual result inspection\n",
    "- Error pattern analysis\n",
    "- Performance bottleneck identification\n",
    "\n",
    "## Output Organization\n",
    "- Structured metric logging\n",
    "- Visualization generation\n",
    "- Detailed performance reports\n",
    "- Error analysis documentation\n",
    "\n",
    "## Significance\n",
    "This evaluation provides critical insights into the model's reliability for autonomous navigation tasks in peatland environments, where accurate object detection is crucial for safe and efficient operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93f2e1",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd64370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection framework\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# File and path handling\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746171c",
   "metadata": {},
   "source": [
    "## 2. Evaluation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5570551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: training/metrics/detection/finetuned/weights/best.pt\n",
      "Using dataset configuration: data/processed/finetuning_v1/data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Model identification\n",
    "RUN_NAME = \"finetuned\"     # Name of the training run\n",
    "PROJECT_DIR = \"./training/metrics/detection\"       # Base metrics directory\n",
    "\n",
    "# Path configuration\n",
    "MODEL_PATH = Path(PROJECT_DIR) / RUN_NAME / \"weights/best.pt\"     # Best model weights\n",
    "DATA_YAML_PATH = Path(\"./data/processed/finetuning_v1/data.yaml\")    # Dataset config\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "print(f\"Using dataset configuration: {DATA_YAML_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fbeb2a",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17638ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize YOLO model with trained weights\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "print(\"Trained model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8845c15",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation Process\n",
    "\n",
    "### Evaluation Protocol\n",
    "\n",
    "1. **Test Set Validation**\n",
    "   - Independent test set assessment\n",
    "   - Unbiased performance measurement\n",
    "   - Real-world scenario simulation\n",
    "   - Batch processing for efficiency\n",
    "\n",
    "2. **Performance Metrics**\n",
    "   - **Mean Average Precision (mAP)**\n",
    "     * IoU thresholds from 0.50 to 0.95\n",
    "     * Standard COCO evaluation protocol\n",
    "     * Class-specific performance analysis\n",
    "   \n",
    "   - **Precision Metrics**\n",
    "     * Per-class precision curves\n",
    "     * Confidence threshold analysis\n",
    "     * False positive analysis\n",
    "   \n",
    "   - **Recall Metrics**\n",
    "     * Per-class recall curves\n",
    "     * Miss rate analysis\n",
    "     * Scale-based performance\n",
    "\n",
    "3. **Results Documentation**\n",
    "   - Automated metric logging\n",
    "   - Performance visualization\n",
    "   - Statistical analysis\n",
    "   - Error pattern documentation\n",
    "\n",
    "### Generated Artifacts\n",
    "\n",
    "1. **Metric Files**\n",
    "   - Detailed CSV reports\n",
    "   - Performance summaries\n",
    "   - Class-wise statistics\n",
    "\n",
    "2. **Visualizations**\n",
    "   - Precision-Recall curves\n",
    "   - Confusion matrices\n",
    "   - Example detections\n",
    "   - Error case analysis\n",
    "\n",
    "3. **Analysis Tools**\n",
    "   - Confidence analysis\n",
    "   - Scale sensitivity study\n",
    "   - Occlusion impact assessment\n",
    "\n",
    "The evaluation provides comprehensive insights into model behavior and reliability for deployment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1db137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.173 ðŸš€ Python-3.11.8 torch-2.7.1 CPU (Apple M2 Pro)\n",
      "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
      "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 152.9Â±16.9 MB/s, size: 767.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 152.9Â±16.9 MB/s, size: 767.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/stahlma/Desktop/01_Studium/09_Vision_Project/peatland_navigation/data/processed/finetuning_v1/labels/test... 5 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 734.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/stahlma/Desktop/01_Studium/09_Vision_Project/peatland_navigation/data/processed/finetuning_v1/labels/test.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5          9      0.821      0.592      0.796      0.567\n",
      "                 bench          3          3      0.494      0.333      0.665       0.52\n",
      "                  cone          2          2          1      0.943      0.995       0.73\n",
      "                  sign          4          4      0.969        0.5      0.728      0.453\n",
      "Speed: 0.7ms preprocess, 113.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/finetuned_test_evaluation\u001b[0m\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "mAP50-95: 0.5673932811019154\n",
      "mAP50: 0.7961111111111112\n",
      "\n",
      "Detailed metrics and plots saved in 'metrics/detection/finetuned_test_evaluation'\n",
      "                 bench          3          3      0.494      0.333      0.665       0.52\n",
      "                  cone          2          2          1      0.943      0.995       0.73\n",
      "                  sign          4          4      0.969        0.5      0.728      0.453\n",
      "Speed: 0.7ms preprocess, 113.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/finetuned_test_evaluation\u001b[0m\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "mAP50-95: 0.5673932811019154\n",
      "mAP50: 0.7961111111111112\n",
      "\n",
      "Detailed metrics and plots saved in 'metrics/detection/finetuned_test_evaluation'\n"
     ]
    }
   ],
   "source": [
    "# Execute model validation on test set\n",
    "metrics = model.val(\n",
    "    data=str(DATA_YAML_PATH),          # Dataset configuration\n",
    "    split='test',                      # Use test set for evaluation\n",
    "    name=f'{RUN_NAME}_test_evaluation' # Output directory name\n",
    ")\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "\n",
    "# Print key metrics\n",
    "print(f\"mAP50-95: {metrics.box.map}\")    # Mean AP across IoU thresholds\n",
    "print(f\"mAP50: {metrics.box.map50}\")     # AP at IoU=0.50\n",
    "\n",
    "# Indicate results location\n",
    "print(f\"\\nDetailed metrics and plots saved in 'metrics/detection/{RUN_NAME}_test_evaluation'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d9f41",
   "metadata": {},
   "source": [
    "### Results Analysis and Interpretation\n",
    "\n",
    "The evaluation generates a comprehensive set of metrics that provide deep insights into the model's detection capabilities. Understanding these metrics is crucial for assessing model deployment readiness.\n",
    "\n",
    "#### Key Performance Indicators\n",
    "\n",
    "1. **mAP50-95 (Primary Metric)**\n",
    "   - **Definition**: Mean Average Precision across IoU thresholds from 0.50 to 0.95\n",
    "   - **Significance**: \n",
    "     * Comprehensive measure of detection quality\n",
    "     * Accounts for localization accuracy\n",
    "     * Standard COCO evaluation metric\n",
    "   - **Interpretation**:\n",
    "     * Higher values indicate better overall performance\n",
    "     * Considers both precise and loose detections\n",
    "     * More stringent than single-threshold metrics\n",
    "\n",
    "2. **mAP50 (Secondary Metric)**\n",
    "   - **Definition**: Average Precision at IoU threshold of 0.50\n",
    "   - **Significance**:\n",
    "     * Standard benchmark in object detection\n",
    "     * More lenient than mAP50-95\n",
    "     * Useful for applications with lower precision requirements\n",
    "   - **Interpretation**:\n",
    "     * Higher values indicate good general detection\n",
    "     * May miss subtle localization errors\n",
    "     * Suitable for initial model assessment\n",
    "\n",
    "#### Additional Analysis Components\n",
    "\n",
    "1. **Precision-Recall Curves**\n",
    "   - Show detection performance across confidence thresholds\n",
    "   - Identify optimal confidence settings\n",
    "   - Reveal trade-offs between precision and recall\n",
    "\n",
    "2. **Confusion Matrices**\n",
    "   - Highlight class-specific performance\n",
    "   - Identify common misclassifications\n",
    "   - Guide potential model improvements\n",
    "\n",
    "3. **Example Detections**\n",
    "   - Visual validation of model behavior\n",
    "   - Error case documentation\n",
    "   - Performance verification\n",
    "\n",
    "All detailed results are stored in the metrics directory for thorough analysis and documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
