{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40221e1c",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29c86ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stahlma/.pyenv/versions/peatland_navigation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# !pip install torchmetrics\n",
    "from torchmetrics import JaccardIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0678e",
   "metadata": {},
   "source": [
    "## 2. Dataset and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e0db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryPeatlandDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading images and BINARY (Path/BG) masks.\"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        mask = mask.long()\n",
    "        return image, mask\n",
    "\n",
    "# --- Define Transforms for Evaluation ---\n",
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "eval_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb26161",
   "metadata": {},
   "source": [
    "## 3. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5b30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from run: binary_unet_2025-08-03_22-53-32\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = \"binary_unet_2025-08-03_22-53-32\" # <--- CHANGE THIS\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME\n",
    "MODEL_PATH = METRICS_DIR / \"best_binary_model.pth\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Evaluating model from run: {RUN_NAME}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ae4c2",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc6eb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 203 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "BASE_PROCESSED_DIR = Path(\"../data/processed/binary_segmentation\")\n",
    "VAL_IMG_DIR = BASE_PROCESSED_DIR / \"val\" / \"images\"\n",
    "VAL_MASK_DIR = BASE_PROCESSED_DIR / \"val\" / \"masks\"\n",
    "\n",
    "# Note: We'll use the validation set as our test set for this evaluation\n",
    "test_dataset = BinaryPeatlandDataset(\n",
    "    images_dir=VAL_IMG_DIR,\n",
    "    masks_dir=VAL_MASK_DIR,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Loaded {len(test_dataset)} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595ba51",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74008b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=2).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ca2bd",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75e78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████| 51/51 [00:21<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Pixel Accuracy: 95.72%\n",
      "Mean IoU (mIoU): 0.8372\n",
      "\n",
      "IoU per Class:\n",
      "  - Background: 0.9519\n",
      "  - Path: 0.7226\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = [\"Background\", \"Path\"]\n",
    "jaccard = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(DEVICE)\n",
    "total_correct_pixels, total_pixels = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        jaccard.update(preds, masks)\n",
    "        total_correct_pixels += (preds == masks).sum().item()\n",
    "        total_pixels += torch.numel(masks)\n",
    "\n",
    "pixel_accuracy = (total_correct_pixels / total_pixels) * 100\n",
    "iou_per_class = jaccard.compute()\n",
    "mean_iou = iou_per_class.mean()\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Overall Pixel Accuracy: {pixel_accuracy:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {mean_iou:.4f}\")\n",
    "print(\"\\nIoU per Class:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  - {class_name}: {iou_per_class[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40676eb6",
   "metadata": {},
   "source": [
    "## 7. Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae9d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics saved to: metrics/binary_unet_2025-08-03_22-53-32/test_set_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "    'Metric': ['Pixel Accuracy', 'Mean IoU'] + [f'IoU_{name}' for name in CLASS_NAMES],\n",
    "    'Value': [pixel_accuracy, mean_iou.item()] + iou_per_class.cpu().numpy().tolist()\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "output_csv_path = METRICS_DIR / \"test_set_evaluation.csv\"\n",
    "metrics_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nEvaluation metrics saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5c3c6",
   "metadata": {},
   "source": [
    "## 8. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b22f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 visualization samples to: metrics/binary_unet_2025-08-03_22-53-32/visualizations\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset, model, device, num_samples=5):\n",
    "    vis_dir = METRICS_DIR / \"visualizations\"\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    color_map = np.array([[0, 0, 0], [60, 16, 152]], dtype=np.uint8) # BG: Black, Path: Purple\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            image_tensor, gt_mask = dataset[i]\n",
    "            display_image = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            display_image = (display_image * IMAGENET_STD) + IMAGENET_MEAN\n",
    "            display_image = np.clip(display_image, 0, 1)\n",
    "            \n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            gt_mask_color = color_map[gt_mask.cpu().numpy()]\n",
    "            pred_mask_color = color_map[pred_mask]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "            ax[0].imshow(display_image); ax[0].set_title(\"Original Image\"); ax[0].axis('off')\n",
    "            ax[1].imshow(gt_mask_color); ax[1].set_title(\"Ground Truth Mask\"); ax[1].axis('off')\n",
    "            ax[2].imshow(pred_mask_color); ax[2].set_title(\"Model Prediction\"); ax[2].axis('off')\n",
    "            plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Saved {num_samples} visualization samples to: {vis_dir}\")\n",
    "\n",
    "visualize_predictions(test_dataset, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e2483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
