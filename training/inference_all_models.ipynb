{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737f9b4d",
   "metadata": {},
   "source": [
    "# Multi-Model Inference Pipeline for Peatland Navigation\n",
    "\n",
    "## Overview\n",
    "This notebook implements an advanced computer vision pipeline that combines multiple deep learning models to enable comprehensive peatland path analysis and navigation assistance.\n",
    "\n",
    "## Model Components\n",
    "\n",
    "### 1. Binary Segmentation (U-Net)\n",
    "- **Purpose**: Primary path identification\n",
    "- **Architecture**: U-Net with ResNet34 backbone\n",
    "- **Output**: Binary masks differentiating paths from surroundings\n",
    "- **Role**: Initial path proposal generation\n",
    "\n",
    "### 2. Segment Anything Model (SAM)\n",
    "- **Purpose**: Path boundary refinement\n",
    "- **Architecture**: Vision Transformer (ViT)\n",
    "- **Output**: High-precision segmentation masks\n",
    "- **Role**: Enhanced path delineation\n",
    "\n",
    "### 3. Object Detection (YOLOv8)\n",
    "- **Purpose**: Navigation marker identification\n",
    "- **Architecture**: YOLOv8 with custom head\n",
    "- **Output**: Bounding boxes and object classes\n",
    "- **Role**: Context-aware navigation assistance\n",
    "\n",
    "### 4. Depth Estimation (MiDaS)\n",
    "- **Purpose**: Scene depth analysis\n",
    "- **Architecture**: Lightweight MiDaS\n",
    "- **Output**: Dense depth maps\n",
    "- **Role**: Real-world measurements and distance estimation\n",
    "\n",
    "## Pipeline Integration\n",
    "The system combines these models to provide:\n",
    "- Real-time path detection\n",
    "- Precise boundary segmentation\n",
    "- Navigation marker recognition\n",
    "- Path width measurements\n",
    "- Distance estimation to objects\n",
    "\n",
    "This comprehensive approach enables robust navigation assistance in challenging peatland environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2d51d",
   "metadata": {},
   "source": [
    "## 1. Dependencies and Framework Integration\n",
    "\n",
    "### Deep Learning Frameworks\n",
    "1. **PyTorch (`torch`)**\n",
    "   - Core deep learning operations\n",
    "   - GPU acceleration support\n",
    "   - Model deployment infrastructure\n",
    "   - Tensor computations\n",
    "\n",
    "2. **Model-Specific Libraries**\n",
    "   - **ultralytics**: YOLOv8 implementation and inference\n",
    "   - **segment-anything**: Meta's SAM framework\n",
    "   - **segmentation-models-pytorch**: U-Net architecture\n",
    "   - **MiDaS**: Monocular depth estimation\n",
    "\n",
    "### Image Processing\n",
    "1. **Computer Vision**\n",
    "   - **opencv-python (`cv2`)**\n",
    "     * Video I/O operations\n",
    "     * Image processing routines\n",
    "     * Visualization utilities\n",
    "   \n",
    "   - **PIL (Python Imaging Library)**\n",
    "     * Image format handling\n",
    "     * Color space conversions\n",
    "     * Image transformations\n",
    "\n",
    "2. **Data Augmentation**\n",
    "   - **albumentations**\n",
    "     * Image transformations\n",
    "     * Normalization pipelines\n",
    "     * Tensor conversion\n",
    "\n",
    "### Utility Libraries\n",
    "1. **Data Handling**\n",
    "   - **numpy**: Numerical operations\n",
    "   - **pathlib**: Path manipulation\n",
    "   - **collections.deque**: Temporal smoothing\n",
    "\n",
    "2. **Progress Monitoring**\n",
    "   - **tqdm**: Progress bar visualization\n",
    "   - Process monitoring\n",
    "   - Time estimation\n",
    "\n",
    "This comprehensive set of libraries enables efficient implementation of the multi-model inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e731f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stahlma/.pyenv/versions/peatland_navigation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from segment_anything import sam_model_registry, SamPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608182f",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "1. **Path Selection**\n",
    "   - **Binary Segmentation Model**\n",
    "     * U-Net checkpoint selection\n",
    "     * Best performance model identification\n",
    "     * Weight file location\n",
    "   \n",
    "   - **Object Detection Model**\n",
    "     * YOLOv8 fine-tuned weights\n",
    "     * Custom class adaptation\n",
    "     * Inference configuration\n",
    "   \n",
    "   - **SAM Model**\n",
    "     * Pretrained weight selection\n",
    "     * Architecture specification (ViT-B)\n",
    "     * Model registry integration\n",
    "\n",
    "2. **Video Processing Parameters**\n",
    "   - **Input Configuration**\n",
    "     * Source video selection\n",
    "     * Frame rate optimization\n",
    "     * Resolution handling\n",
    "   \n",
    "   - **Output Settings**\n",
    "     * Destination path configuration\n",
    "     * Format specifications\n",
    "     * Quality parameters\n",
    "\n",
    "3. **Inference Parameters**\n",
    "   - **Detection Thresholds**\n",
    "     * Confidence threshold optimization\n",
    "     * Minimum path area validation\n",
    "     * False positive filtering\n",
    "   \n",
    "   - **Hardware Utilization**\n",
    "     * Device selection logic\n",
    "     * GPU/CPU optimization\n",
    "     * Memory management\n",
    "\n",
    "4. **Path Analysis Settings**\n",
    "   - **Smoothing Parameters**\n",
    "     * Temporal window size\n",
    "     * Weight distribution\n",
    "     * Update frequency\n",
    "\n",
    "   - **Measurement Calibration**\n",
    "     * Distance scaling factors\n",
    "     * Camera parameters\n",
    "     * Real-world unit conversion\n",
    "\n",
    "These configuration parameters are crucial for:\n",
    "- Optimal model performance\n",
    "- Efficient resource utilization\n",
    "- Accurate measurements\n",
    "- Reliable real-time processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02f77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Loading Segmentation Model: training/metrics/segmentation/binary_unet_2025-08-03_22-53-32/best_binary_model.pth\n",
      "Loading Detection Model: training/metrics/detection/finetuned/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Model Paths ---\n",
    "SEGMENTATION_RUN_NAME = \"binary_unet_2025-08-03_22-53-32\" \n",
    "DETECTION_RUN_NAME = \"finetuned\" \n",
    "SAM_CHECKPOINT_PATH = Path(\"./training/sam_vit_b.pth\") \n",
    "SAM_MODEL_TYPE = \"vit_b\"\n",
    "\n",
    "# --- Video Configuration ---\n",
    "VIDEO_FILENAME = \"Clip_3_35s.mp4\" \n",
    "FPS_REDUCTION_FACTOR = 6 # Process 1 frame every N frames (60fps / 6 = 10fps)\n",
    "\n",
    "# --- Detection Configuration ---\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "MIN_PATH_AREA_THRESHOLD = 2000 # Minimum area of the path to be considered valid\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "SEG_MODEL_PATH = Path(\"./training/metrics/segmentation\") / SEGMENTATION_RUN_NAME / \"best_binary_model.pth\"\n",
    "DET_MODEL_PATH = Path(\"./training/metrics/detection\") / DETECTION_RUN_NAME / \"weights/best.pt\"\n",
    "VIDEO_PATH = Path(\"./data/video/splits\") / VIDEO_FILENAME\n",
    "OUTPUT_DIR = Path(\"final_video_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / f\"showcase_{VIDEO_FILENAME}\"\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Loading Segmentation Model: {SEG_MODEL_PATH}\")\n",
    "print(f\"Loading Detection Model: {DET_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76cbf9",
   "metadata": {},
   "source": [
    "## 3. Model Initialization and Architecture Setup\n",
    "\n",
    "### 1. Binary Segmentation (U-Net)\n",
    "\n",
    "**Architecture Components:**\n",
    "- ResNet34 encoder for feature extraction\n",
    "- Skip connections for detail preservation\n",
    "- Binary classification head\n",
    "- Custom preprocessing pipeline\n",
    "\n",
    "**Configuration Details:**\n",
    "- Input channels: 3 (RGB)\n",
    "- Output classes: 2 (path/non-path)\n",
    "- Image size: 480x640\n",
    "- Normalization: ImageNet statistics\n",
    "\n",
    "### 2. Segment Anything Model (SAM)\n",
    "\n",
    "**Implementation Details:**\n",
    "- ViT-B architecture variant\n",
    "- Point-prompt based refinement\n",
    "- Multi-mask output capability\n",
    "- High-precision boundary detection\n",
    "\n",
    "**Initialization Process:**\n",
    "- Model registry integration\n",
    "- Weight loading and verification\n",
    "- Device optimization\n",
    "- Predictor setup\n",
    "\n",
    "### 3. Object Detection (YOLOv8)\n",
    "\n",
    "**Model Configuration:**\n",
    "- Fine-tuned weights loading\n",
    "- Class mapping verification\n",
    "- Confidence threshold setup\n",
    "- Non-maximum suppression settings\n",
    "\n",
    "**Detection Classes:**\n",
    "- Bench markers\n",
    "- Navigation cones\n",
    "- Class ID mapping for visualization\n",
    "\n",
    "### 4. Depth Estimation (MiDaS)\n",
    "\n",
    "**Model Variant:**\n",
    "- Small architecture for efficiency\n",
    "- Real-time processing capability\n",
    "- Monocular depth estimation\n",
    "- Custom transform pipeline\n",
    "\n",
    "**Processing Setup:**\n",
    "- Input normalization\n",
    "- Resolution adaptation\n",
    "- Output scaling\n",
    "- Device optimization\n",
    "\n",
    "Each model is configured for optimal performance while maintaining real-time processing capabilities in the integrated pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa6fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation model loaded.\n",
      "SAM model loaded.\n",
      "Detection model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/Users/stahlma/.pyenv/versions/peatland_navigation/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "# --- Segmentation U-Net ---\n",
    "seg_model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=2).to(DEVICE)\n",
    "seg_model.load_state_dict(torch.load(SEG_MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "seg_model.eval()\n",
    "seg_transform = A.Compose([\n",
    "    A.Resize(height=480, width=640),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "print(\"Segmentation model loaded.\")\n",
    "\n",
    "# --- SAM ---\n",
    "sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "print(\"SAM model loaded.\")\n",
    "\n",
    "# --- Detection YOLOv8 ---\n",
    "detection_model = YOLO(DET_MODEL_PATH)\n",
    "BENCH_CLASS_ID = [k for k, v in detection_model.names.items() if v == 'bench'][0]\n",
    "CONE_CLASS_ID = [k for k, v in detection_model.names.items() if v == 'cone'][0]\n",
    "print(\"Detection model loaded.\")\n",
    "\n",
    "# --- Depth MiDaS ---\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\", trust_repo=True)\n",
    "midas.to(DEVICE)\n",
    "midas.eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
    "midas_transform = midas_transforms.small_transform\n",
    "print(\"Depth model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d775f",
   "metadata": {},
   "source": [
    "## 4. Core Processing Functions\n",
    "\n",
    "### Path Analysis Functions\n",
    "\n",
    "1. **Path Mask Generation (`get_smoothed_path_mask`)**\n",
    "   \n",
    "   **Purpose:**\n",
    "   - Generate temporally consistent path segmentation\n",
    "   - Apply U-Net for initial prediction\n",
    "   - Implement weighted temporal smoothing\n",
    "   \n",
    "   **Implementation Details:**\n",
    "   - Moving average over multiple frames\n",
    "   - Weighted contribution of historical predictions\n",
    "   - Resolution matching and interpolation\n",
    "\n",
    "2. **Depth Map Generation (`get_depth_map`)**\n",
    "   \n",
    "   **Purpose:**\n",
    "   - Generate dense depth maps\n",
    "   - Enable distance measurements\n",
    "   - Support path width estimation\n",
    "   \n",
    "   **Technical Components:**\n",
    "   - MiDaS inference pipeline\n",
    "   - Resolution adaptation\n",
    "   - Depth map normalization\n",
    "   - Output calibration\n",
    "\n",
    "3. **Path Point Extraction (`mask_to_prompt_points`)**\n",
    "   \n",
    "   **Purpose:**\n",
    "   - Extract key points for SAM refinement\n",
    "   - Validate path detection confidence\n",
    "   - Generate prompt coordinates\n",
    "   \n",
    "   **Processing Steps:**\n",
    "   - Contour detection and filtering\n",
    "   - Area-based validation\n",
    "   - Centroid calculation\n",
    "   - Multi-point selection\n",
    "\n",
    "4. **Width Measurement (`estimate_width_in_meters`)**\n",
    "   \n",
    "   **Purpose:**\n",
    "   - Convert pixel measurements to real-world units\n",
    "   - Account for perspective effects\n",
    "   - Provide accurate path width estimates\n",
    "   \n",
    "   **Implementation Details:**\n",
    "   - Pinhole camera model implementation\n",
    "   - Focal length calculation\n",
    "   - Distance-based scaling\n",
    "   - Calibration factor application\n",
    "\n",
    "### Function Integration\n",
    "\n",
    "These utility functions form a coordinated processing pipeline:\n",
    "1. Initial path detection (U-Net)\n",
    "2. Temporal consistency (Smoothing)\n",
    "3. Boundary refinement (SAM)\n",
    "4. Measurement calculation (Depth + Geometry)\n",
    "\n",
    "The functions are designed for:\n",
    "- Efficient processing\n",
    "- Robust error handling\n",
    "- Real-time performance\n",
    "- Accurate measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521b4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COLOR_BGR = (227, 124, 240)  # A pleasing light purple\n",
    "PATH_CLASS_ID = 1\n",
    "DISTANCE_CALIBRATION_FACTOR = 500 \n",
    "\n",
    "def get_smoothed_path_mask(frame, logits_history):\n",
    "    \"\"\"Gets U-Net prediction and applies a weighted moving average.\"\"\"\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = seg_transform(image=image_rgb)['image'].unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        current_logits = seg_model(image_tensor)\n",
    "    \n",
    "    logits_history.append(current_logits)\n",
    "    weights = torch.linspace(0.1, 1.0, len(logits_history)).to(DEVICE)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    weighted_sum = torch.zeros_like(current_logits)\n",
    "    for i, logits in enumerate(logits_history):\n",
    "        weighted_sum += logits * weights[i]\n",
    "        \n",
    "    preds = torch.argmax(weighted_sum, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    return cv2.resize(preds, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def get_depth_map(frame):\n",
    "    \"\"\"Gets MiDaS depth prediction.\"\"\"\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_batch = midas_transform(img_rgb).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "    return torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1), size=img_rgb.shape[:2], mode=\"bicubic\", align_corners=False\n",
    "    ).squeeze().cpu().numpy()\n",
    "\n",
    "def mask_to_prompt_points(mask, max_points=3):\n",
    "    \"\"\"Finds the largest contours in the U-Net mask and returns their center points.\"\"\"\n",
    "    contours, _ = cv2.findContours((mask == PATH_CLASS_ID).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours: return None, 0\n",
    "    \n",
    "    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    largest_contour_area = cv2.contourArea(sorted_contours[0])\n",
    "    \n",
    "    points = []\n",
    "    for contour in sorted_contours[:max_points]:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            points.append([int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"])])\n",
    "            \n",
    "    return np.array(points) if points else None, largest_contour_area\n",
    "\n",
    "def estimate_width_in_meters(pixel_width, distance_m, frame_width, fov_degrees=70):\n",
    "    \"\"\"Estimates real-world width using the pinhole camera model.\"\"\"\n",
    "    # Calculate the camera's focal length in pixels\n",
    "    fov_radians = np.deg2rad(fov_degrees)\n",
    "    focal_length_pixels = (frame_width / 2) / np.tan(fov_radians / 2)\n",
    "    \n",
    "    # Calculate the size of one pixel in meters at the given distance\n",
    "    meters_per_pixel = distance_m / focal_length_pixels\n",
    "    \n",
    "    # Estimate the real-world width\n",
    "    width_meters = pixel_width * meters_per_pixel * 1.5  # Adjust factor based on visual calibration\n",
    "    return width_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d6edb",
   "metadata": {},
   "source": [
    "## 5. Video Processing Pipeline\n",
    "\n",
    "### Input Processing Framework\n",
    "\n",
    "1. **Video Stream Management**\n",
    "   - Frame extraction with controlled FPS\n",
    "   - Resolution maintenance\n",
    "   - Format standardization\n",
    "   - Progress monitoring\n",
    "\n",
    "2. **Frame Processing**\n",
    "   - Color space conversions\n",
    "   - Resolution adaptation\n",
    "   - Batch preparation\n",
    "   - Memory management\n",
    "\n",
    "### Multi-Model Inference Pipeline\n",
    "\n",
    "1. **Path Segmentation Process**\n",
    "   \n",
    "   **Initial Detection:**\n",
    "   - U-Net inference\n",
    "   - Confidence validation\n",
    "   - Area thresholding\n",
    "   - Temporal consistency check\n",
    "   \n",
    "   **Refinement Stage:**\n",
    "   - SAM prompt generation\n",
    "   - Boundary refinement\n",
    "   - IoU-based mask selection\n",
    "   - Temporal smoothing\n",
    "\n",
    "2. **Object Detection Integration**\n",
    "   \n",
    "   **Detection Process:**\n",
    "   - YOLOv8 inference\n",
    "   - Confidence filtering\n",
    "   - Class identification\n",
    "   - Non-maximum suppression\n",
    "   \n",
    "   **Distance Analysis:**\n",
    "   - Depth map integration\n",
    "   - Distance calculation\n",
    "   - Real-world scaling\n",
    "   - Measurement annotation\n",
    "\n",
    "3. **Measurement Pipeline**\n",
    "   \n",
    "   **Path Analysis:**\n",
    "   - Width measurement points\n",
    "   - Depth integration\n",
    "   - Real-world conversion\n",
    "   - Temporal stability\n",
    "   \n",
    "   **Object Relations:**\n",
    "   - Distance to path\n",
    "   - Inter-object distances\n",
    "   - Spatial relationships\n",
    "   - Navigation context\n",
    "\n",
    "### Visualization Framework\n",
    "\n",
    "1. **Path Visualization**\n",
    "   - Contour overlay\n",
    "   - Transparency management\n",
    "   - Color coding\n",
    "   - Width indicators\n",
    "\n",
    "2. **Object Annotation**\n",
    "   - Bounding box rendering\n",
    "   - Class labels\n",
    "   - Confidence scores\n",
    "   - Distance measurements\n",
    "\n",
    "3. **Measurement Display**\n",
    "   - Width measurements\n",
    "   - Distance annotations\n",
    "   - Unit conversion\n",
    "   - Text placement\n",
    "\n",
    "### Pipeline Integration\n",
    "\n",
    "The system maintains:\n",
    "- Frame-to-frame consistency\n",
    "- Real-time processing capability\n",
    "- Memory efficiency\n",
    "- Robust error handling\n",
    "\n",
    "This comprehensive pipeline enables detailed scene understanding and measurement for navigation assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7da619d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating final video: 100%|██████████| 1065/1065 [02:12<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final showcase video saved to: final_video_output/showcase_Clip_3_35s.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error opening video file {VIDEO_PATH}\")\n",
    "else:\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(OUTPUT_VIDEO_PATH), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    last_refined_mask, last_boxes, last_depth_map, last_known_points = None, [], None, None\n",
    "    frame_count = 0\n",
    "    logits_history = deque(maxlen=10)\n",
    "\n",
    "    for _ in tqdm(range(total_frames), desc=\"Creating final video\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        if frame_count % FPS_REDUCTION_FACTOR == 0:\n",
    "            unet_mask = get_smoothed_path_mask(frame, logits_history)\n",
    "            current_points, path_area = mask_to_prompt_points(unet_mask)\n",
    "            \n",
    "            # --- UPDATED: Confidence check for path prediction ---\n",
    "            if path_area > MIN_PATH_AREA_THRESHOLD:\n",
    "                last_known_points = current_points\n",
    "            else: # If path is too small or non-existent, don't use a prompt\n",
    "                last_known_points = None\n",
    "\n",
    "            if last_known_points is not None:\n",
    "                sam_predictor.set_image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                masks_sam, _, _ = sam_predictor.predict(\n",
    "                    point_coords=last_known_points,\n",
    "                    point_labels=np.ones(len(last_known_points)),\n",
    "                    multimask_output=True,\n",
    "                )\n",
    "                \n",
    "                best_iou, best_mask = -1, None\n",
    "                unet_path_mask_bool = (unet_mask == PATH_CLASS_ID)\n",
    "                for mask in masks_sam:\n",
    "                    intersection = np.logical_and(unet_path_mask_bool, mask).sum()\n",
    "                    union = np.logical_or(unet_path_mask_bool, mask).sum()\n",
    "                    iou = intersection / union if union > 0 else 0\n",
    "                    if iou > best_iou:\n",
    "                        best_iou, best_mask = iou, mask\n",
    "                last_refined_mask = best_mask\n",
    "            else:\n",
    "                last_refined_mask = None\n",
    "            \n",
    "            last_boxes = detection_model(frame, verbose=False)[0].boxes\n",
    "            last_depth_map = get_depth_map(frame)\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        if last_refined_mask is not None:\n",
    "            contours, _ = cv2.findContours(last_refined_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(annotated_frame, contours, -1, PATH_COLOR_BGR, 3)\n",
    "\n",
    "            width_measure_y = int(frame_height * 0.8)\n",
    "            line = last_refined_mask[width_measure_y, :]\n",
    "            path_pixels = np.where(line)[0]\n",
    "            # if len(path_pixels) > 1 and last_depth_map is not None:\n",
    "            #     x_start, x_end = path_pixels[0], path_pixels[-1]\n",
    "            #     pixel_width = x_end - x_start\n",
    "                \n",
    "            #     depth_at_line = np.median(last_depth_map[width_measure_y, x_start:x_end])\n",
    "            #     distance_m = DISTANCE_CALIBRATION_FACTOR / depth_at_line\n",
    "                \n",
    "            #     path_width_meters = estimate_width_in_meters(pixel_width, distance_m, frame_width)\n",
    "\n",
    "            #     cv2.line(annotated_frame, (x_start, width_measure_y), (x_end, width_measure_y), (255, 255, 0), 3)\n",
    "            #     label = f\"Path Width: {path_width_meters:.2f}m\"\n",
    "            #     cv2.putText(annotated_frame, label, (x_start, width_measure_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "        if last_boxes is not None and last_depth_map is not None:\n",
    "            for box in last_boxes:\n",
    "                if box.conf >= CONFIDENCE_THRESHOLD:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    class_id = int(box.cls[0])\n",
    "                    class_name = detection_model.names[class_id]\n",
    "                    \n",
    "                    label = f\"{class_name} ({box.conf[0]:.2f})\"\n",
    "                    color = (0, 255, 0)\n",
    "\n",
    "                    box_depth = last_depth_map[y1:y2, x1:x2]\n",
    "                    if box_depth.size > 0:\n",
    "                        distance = DISTANCE_CALIBRATION_FACTOR / np.median(box_depth)\n",
    "                        label += f\" | Dist: {distance:.1f}m\"\n",
    "\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(annotated_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "        frame_count += 1\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nFinal showcase video saved to: {OUTPUT_VIDEO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c984a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
