{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98c3f71",
   "metadata": {},
   "source": [
    "# Multi-Model Segmentation Inference\n",
    "\n",
    "This notebook implements video segmentation using two different architectures:\n",
    "\n",
    "1. **DinoV2-Based Model**:\n",
    "   - Self-supervised pre-training\n",
    "   - Vision Transformer backbone\n",
    "   - Multi-class segmentation\n",
    "\n",
    "2. **U-Net Model**:\n",
    "   - CNN-based architecture\n",
    "   - ResNet34 encoder\n",
    "   - Standard segmentation approach\n",
    "\n",
    "The notebook supports switching between models and provides consistent visualization for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0103d0",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a666723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import Dinov2Model, AutoImageProcessor\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4ac1a",
   "metadata": {},
   "source": [
    "## 2. Model Architecture\n",
    "\n",
    "### DinoV2 Architecture\n",
    "The `DinoV2ForSemanticSegmentation` class implements:\n",
    "- Frozen DinoV2-base backbone\n",
    "- Custom decoder head design\n",
    "- Multi-scale feature processing\n",
    "- Progressive upsampling\n",
    "\n",
    "Key components:\n",
    "1. **Feature Extraction**:\n",
    "   - Pre-trained ViT-B backbone\n",
    "   - Patch token processing\n",
    "   - Hidden state utilization\n",
    "\n",
    "2. **Decoder Design**:\n",
    "   - Channel reduction (768→256→128→64)\n",
    "   - Progressive upsampling\n",
    "   - Bilinear interpolation\n",
    "   - Final classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09baa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_IMAGE_SIZE = 224\n",
    "\n",
    "class DinoV2ForSemanticSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(DinoV2ForSemanticSegmentation, self).__init__()\n",
    "        self.dinov2 = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.dinov2(pixel_values, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        patch_tokens = last_hidden_state[:, 1:, :]\n",
    "        batch_size, seq_len, num_channels = patch_tokens.shape\n",
    "        height = width = int(seq_len**0.5)\n",
    "        feature_map = patch_tokens.permute(0, 2, 1).contiguous().reshape(batch_size, num_channels, height, width)\n",
    "        logits = self.head(feature_map)\n",
    "        final_logits = nn.functional.interpolate(logits, size=(DINO_IMAGE_SIZE, DINO_IMAGE_SIZE), mode='bilinear', align_corners=False)\n",
    "        return final_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d37a35",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "### Model Selection and Setup\n",
    "- Choice between DinoV2 and U-Net architectures\n",
    "- Run-specific model loading\n",
    "- Automatic output organization\n",
    "\n",
    "### Path Configuration\n",
    "1. **Model Paths**:\n",
    "   - Architecture-specific model files\n",
    "   - Run-based organization\n",
    "   - Automatic path resolution\n",
    "\n",
    "2. **Video Settings**:\n",
    "   - Input/output paths\n",
    "   - Automated directory creation\n",
    "   - Consistent naming conventions\n",
    "\n",
    "3. **Hardware Setup**:\n",
    "   - CUDA/MPS/CPU detection\n",
    "   - Device optimization\n",
    "   - Resource management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a809bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_TYPE: unet\n",
      "Loading model from run: 2025-08-03_15-56-59\n",
      "Processing video: ../data/video/splits/Clip_1_42s.mp4\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- CHOOSE THE MODEL TYPE ---\n",
    "MODEL_TYPE = \"unet\"  # Options: \"dino\" or \"unet\"\n",
    "\n",
    "# --- CHOOSE THE RUN AND VIDEO ---\n",
    "# If MODEL_TYPE is \"dino\", use a DinoV2 run name\n",
    "if MODEL_TYPE == \"dino\":\n",
    "    RUN_NAME = \"dinov2_2025-08-03_19-45-00\" # <--- DinoV2 run folder name\n",
    "    MODEL_FILENAME = \"best_model.pth\"\n",
    "# If MODEL_TYPE is \"unet\", use a U-Net run name\n",
    "else:\n",
    "    RUN_NAME = \"2025-08-03_15-56-59\" # <--- U-Net run folder name\n",
    "    MODEL_FILENAME = \"peatland_segmentation_model.pth\"\n",
    "\n",
    "VIDEO_FILENAME = \"Clip_1_42s.mp4\" # <--- Video you want to process\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME\n",
    "MODEL_PATH = METRICS_DIR / MODEL_FILENAME\n",
    "VIDEO_PATH = Path(\"../data/video/splits\") / VIDEO_FILENAME\n",
    "OUTPUT_DIR = METRICS_DIR / \"video_predictions\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / f\"predicted_{MODEL_TYPE}_{VIDEO_FILENAME}\"\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"MODEL_TYPE: {MODEL_TYPE}\")\n",
    "print(f\"Loading model from run: {RUN_NAME}\")\n",
    "print(f\"Processing video: {VIDEO_PATH}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc110db",
   "metadata": {},
   "source": [
    "## 4. Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d9bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "if MODEL_TYPE == \"dino\":\n",
    "    DINO_IMAGE_SIZE = 224\n",
    "    processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "    model = DinoV2ForSemanticSegmentation(num_classes=5).to(DEVICE)\n",
    "else: # unet\n",
    "    IMG_HEIGHT = 480\n",
    "    IMG_WIDTH = 640\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "    processor = A.Compose([\n",
    "        A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
    "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=5).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68615f9b",
   "metadata": {},
   "source": [
    "## 5. Visualization and Processing Functions\n",
    "\n",
    "### Color Scheme\n",
    "- **PATH**: Purple (152, 16, 60)\n",
    "- **NATURAL_GROUND**: Pink (246, 41, 132)\n",
    "- **TREE**: Light Blue (228, 193, 110)\n",
    "- **VEGETATION**: Yellow (58, 221, 254)\n",
    "\n",
    "### Processing Pipeline\n",
    "1. **Frame Processing**:\n",
    "   - RGB conversion\n",
    "   - Model-specific preprocessing\n",
    "   - Batch dimension handling\n",
    "\n",
    "2. **Mask Generation**:\n",
    "   - Model inference\n",
    "   - Argmax prediction\n",
    "   - Resolution matching\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Class-based coloring\n",
    "   - Contour drawing\n",
    "   - Transparent overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d89010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS = {\n",
    "    \"PATH\": (152, 16, 60), \"NATURAL_GROUND\": (246, 41, 132),\n",
    "    \"TREE\": (228, 193, 110), \"VEGETATION\": (58, 221, 254),\n",
    "}\n",
    "CLASS_IDS = {\"PATH\": 0, \"NATURAL_GROUND\": 1, \"TREE\": 2, \"VEGETATION\": 3, \"IGNORE\": 4}\n",
    "\n",
    "def get_prediction_mask(frame: np.ndarray, model, processor, model_type, device):\n",
    "    \"\"\"Takes a video frame, runs inference, and returns the raw prediction mask.\"\"\"\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if model_type == \"dino\":\n",
    "        image_pil = Image.fromarray(image_rgb)\n",
    "        pixel_values = processor(image_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    else: # unet\n",
    "        augmented = processor(image=image_rgb)\n",
    "        pixel_values = augmented['image'].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        \n",
    "    preds = torch.argmax(outputs, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    pred_resized = cv2.resize(preds, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return pred_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167e151",
   "metadata": {},
   "source": [
    "## 6. Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e381043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1287 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [02:00<00:00, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete.\n",
      "Output video saved to: metrics/2025-08-03_15-56-59/video_predictions/predicted_unet_Clip_1_42s.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {VIDEO_PATH}\")\n",
    "else:\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(OUTPUT_VIDEO_PATH), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing {total_frames} frames...\")\n",
    "    \n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # 1. Get the raw prediction mask from the model\n",
    "        pred_mask = get_prediction_mask(frame, model, processor, MODEL_TYPE, DEVICE)\n",
    "        \n",
    "        # 2. Create a transparent overlay\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        \n",
    "        # 3. Fill the PATH area with a semi-transparent color\n",
    "        path_mask = (pred_mask == CLASS_IDS[\"PATH\"])\n",
    "        overlay[path_mask] = CLASS_COLORS[\"PATH\"]\n",
    "        \n",
    "        # 4. Find and draw contours for VEGETATION\n",
    "        veg_mask = (pred_mask == CLASS_IDS[\"VEGETATION\"]).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(veg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, CLASS_COLORS[\"VEGETATION\"], 2) # 2px thick line\n",
    "        \n",
    "        # 5. Find and draw contours for TREES\n",
    "        tree_mask = (pred_mask == CLASS_IDS[\"TREE\"]).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(tree_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, CLASS_COLORS[\"TREE\"], 2) # 2px thick line\n",
    "\n",
    "        # 6. Blend the overlay with the original frame\n",
    "        final_frame = cv2.addWeighted(frame, 1.0, overlay, 0.6, 0)\n",
    "        \n",
    "        out.write(final_frame)\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
