{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e731f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stahlma/.pyenv/versions/peatland_navigation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from segment_anything import sam_model_registry, SamPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02f77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Loading Segmentation Model: metrics/segmentation/binary_unet_2025-08-03_22-53-32/best_binary_model.pth\n",
      "Loading Detection Model: metrics/detection/finetuned/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Model Paths ---\n",
    "SEGMENTATION_RUN_NAME = \"binary_unet_2025-08-03_22-53-32\" # <--- Your best binary U-Net run\n",
    "DETECTION_RUN_NAME = \"finetuned\" # <--- Your best fine-tuned detector run\n",
    "SAM_CHECKPOINT_PATH = Path(\"sam_vit_b.pth\") # <--- Path to your downloaded SAM model\n",
    "SAM_MODEL_TYPE = \"vit_b\"\n",
    "\n",
    "# --- Video Configuration ---\n",
    "VIDEO_FILENAME = \"Clip_2_34s.mp4\" \n",
    "FPS_REDUCTION_FACTOR = 6 # Process 1 frame every N frames (60fps / 6 = 10fps)\n",
    "\n",
    "# --- Detection Configuration ---\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "MIN_PATH_AREA_THRESHOLD = 2000 # Minimum area of the path to be considered valid\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "SEG_MODEL_PATH = Path(\"metrics/segmentation\") / SEGMENTATION_RUN_NAME / \"best_binary_model.pth\"\n",
    "DET_MODEL_PATH = Path(\"metrics/detection\") / DETECTION_RUN_NAME / \"weights/best.pt\"\n",
    "VIDEO_PATH = Path(\"../data/video/splits\") / VIDEO_FILENAME\n",
    "OUTPUT_DIR = Path(\"final_video_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / f\"showcase_{VIDEO_FILENAME}\"\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Loading Segmentation Model: {SEG_MODEL_PATH}\")\n",
    "print(f\"Loading Detection Model: {DET_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa6fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation model loaded.\n",
      "SAM model loaded.\n",
      "Detection model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /Users/stahlma/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n",
      "Depth model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "# --- Segmentation U-Net ---\n",
    "seg_model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=2).to(DEVICE)\n",
    "seg_model.load_state_dict(torch.load(SEG_MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "seg_model.eval()\n",
    "seg_transform = A.Compose([\n",
    "    A.Resize(height=480, width=640),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "print(\"Segmentation model loaded.\")\n",
    "\n",
    "# --- SAM ---\n",
    "sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "print(\"SAM model loaded.\")\n",
    "\n",
    "# --- Detection YOLOv8 ---\n",
    "detection_model = YOLO(DET_MODEL_PATH)\n",
    "BENCH_CLASS_ID = [k for k, v in detection_model.names.items() if v == 'bench'][0]\n",
    "CONE_CLASS_ID = [k for k, v in detection_model.names.items() if v == 'cone'][0]\n",
    "print(\"Detection model loaded.\")\n",
    "\n",
    "# --- Depth MiDaS ---\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\", trust_repo=True)\n",
    "midas.to(DEVICE)\n",
    "midas.eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
    "midas_transform = midas_transforms.small_transform\n",
    "print(\"Depth model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "521b4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COLOR_BGR = (227, 124, 240)  # A pleasing light purple\n",
    "PATH_CLASS_ID = 1\n",
    "DISTANCE_CALIBRATION_FACTOR = 500 \n",
    "\n",
    "def get_smoothed_path_mask(frame, logits_history):\n",
    "    \"\"\"Gets U-Net prediction and applies a weighted moving average.\"\"\"\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = seg_transform(image=image_rgb)['image'].unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        current_logits = seg_model(image_tensor)\n",
    "    \n",
    "    logits_history.append(current_logits)\n",
    "    weights = torch.linspace(0.1, 1.0, len(logits_history)).to(DEVICE)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    weighted_sum = torch.zeros_like(current_logits)\n",
    "    for i, logits in enumerate(logits_history):\n",
    "        weighted_sum += logits * weights[i]\n",
    "        \n",
    "    preds = torch.argmax(weighted_sum, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    return cv2.resize(preds, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def get_depth_map(frame):\n",
    "    \"\"\"Gets MiDaS depth prediction.\"\"\"\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_batch = midas_transform(img_rgb).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "    return torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1), size=img_rgb.shape[:2], mode=\"bicubic\", align_corners=False\n",
    "    ).squeeze().cpu().numpy()\n",
    "\n",
    "def mask_to_prompt_points(mask, max_points=3):\n",
    "    \"\"\"Finds the largest contours in the U-Net mask and returns their center points.\"\"\"\n",
    "    contours, _ = cv2.findContours((mask == PATH_CLASS_ID).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours: return None, 0\n",
    "    \n",
    "    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    largest_contour_area = cv2.contourArea(sorted_contours[0])\n",
    "    \n",
    "    points = []\n",
    "    for contour in sorted_contours[:max_points]:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            points.append([int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"])])\n",
    "            \n",
    "    return np.array(points) if points else None, largest_contour_area\n",
    "\n",
    "def estimate_width_in_meters(pixel_width, distance_m, frame_width, fov_degrees=70):\n",
    "    \"\"\"Estimates real-world width using the pinhole camera model.\"\"\"\n",
    "    # Calculate the camera's focal length in pixels\n",
    "    fov_radians = np.deg2rad(fov_degrees)\n",
    "    focal_length_pixels = (frame_width / 2) / np.tan(fov_radians / 2)\n",
    "    \n",
    "    # Calculate the size of one pixel in meters at the given distance\n",
    "    meters_per_pixel = distance_m / focal_length_pixels\n",
    "    \n",
    "    # Estimate the real-world width\n",
    "    width_meters = pixel_width * meters_per_pixel * 1.5  # Adjust factor based on visual calibration\n",
    "    return width_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da619d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating final video: 100%|██████████| 1032/1032 [02:10<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final showcase video saved to: final_video_output/showcase_Clip_2_34s.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error opening video file {VIDEO_PATH}\")\n",
    "else:\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(OUTPUT_VIDEO_PATH), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    last_refined_mask, last_boxes, last_depth_map, last_known_points = None, [], None, None\n",
    "    frame_count = 0\n",
    "    logits_history = deque(maxlen=10)\n",
    "\n",
    "    for _ in tqdm(range(total_frames), desc=\"Creating final video\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        if frame_count % FPS_REDUCTION_FACTOR == 0:\n",
    "            unet_mask = get_smoothed_path_mask(frame, logits_history)\n",
    "            current_points, path_area = mask_to_prompt_points(unet_mask)\n",
    "            \n",
    "            # --- UPDATED: Confidence check for path prediction ---\n",
    "            if path_area > MIN_PATH_AREA_THRESHOLD:\n",
    "                last_known_points = current_points\n",
    "            else: # If path is too small or non-existent, don't use a prompt\n",
    "                last_known_points = None\n",
    "\n",
    "            if last_known_points is not None:\n",
    "                sam_predictor.set_image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                masks_sam, _, _ = sam_predictor.predict(\n",
    "                    point_coords=last_known_points,\n",
    "                    point_labels=np.ones(len(last_known_points)),\n",
    "                    multimask_output=True,\n",
    "                )\n",
    "                \n",
    "                best_iou, best_mask = -1, None\n",
    "                unet_path_mask_bool = (unet_mask == PATH_CLASS_ID)\n",
    "                for mask in masks_sam:\n",
    "                    intersection = np.logical_and(unet_path_mask_bool, mask).sum()\n",
    "                    union = np.logical_or(unet_path_mask_bool, mask).sum()\n",
    "                    iou = intersection / union if union > 0 else 0\n",
    "                    if iou > best_iou:\n",
    "                        best_iou, best_mask = iou, mask\n",
    "                last_refined_mask = best_mask\n",
    "            else:\n",
    "                last_refined_mask = None\n",
    "            \n",
    "            last_boxes = detection_model(frame, verbose=False)[0].boxes\n",
    "            last_depth_map = get_depth_map(frame)\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        if last_refined_mask is not None:\n",
    "            contours, _ = cv2.findContours(last_refined_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(annotated_frame, contours, -1, PATH_COLOR_BGR, 3)\n",
    "\n",
    "            width_measure_y = int(frame_height * 0.8)\n",
    "            line = last_refined_mask[width_measure_y, :]\n",
    "            path_pixels = np.where(line)[0]\n",
    "            if len(path_pixels) > 1 and last_depth_map is not None:\n",
    "                x_start, x_end = path_pixels[0], path_pixels[-1]\n",
    "                pixel_width = x_end - x_start\n",
    "                \n",
    "                depth_at_line = np.median(last_depth_map[width_measure_y, x_start:x_end])\n",
    "                distance_m = DISTANCE_CALIBRATION_FACTOR / depth_at_line\n",
    "                \n",
    "                path_width_meters = estimate_width_in_meters(pixel_width, distance_m, frame_width)\n",
    "\n",
    "                cv2.line(annotated_frame, (x_start, width_measure_y), (x_end, width_measure_y), (255, 255, 0), 3)\n",
    "                label = f\"Path Width: {path_width_meters:.2f}m\"\n",
    "                cv2.putText(annotated_frame, label, (x_start, width_measure_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "        if last_boxes is not None and last_depth_map is not None:\n",
    "            for box in last_boxes:\n",
    "                if box.conf >= CONFIDENCE_THRESHOLD:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    class_id = int(box.cls[0])\n",
    "                    class_name = detection_model.names[class_id]\n",
    "                    \n",
    "                    label = f\"{class_name} ({box.conf[0]:.2f})\"\n",
    "                    color = (0, 255, 0)\n",
    "\n",
    "                    box_depth = last_depth_map[y1:y2, x1:x2]\n",
    "                    if box_depth.size > 0:\n",
    "                        distance = DISTANCE_CALIBRATION_FACTOR / np.median(box_depth)\n",
    "                        label += f\" | Dist: {distance:.1f}m\"\n",
    "\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(annotated_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "        frame_count += 1\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nFinal showcase video saved to: {OUTPUT_VIDEO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a7891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
