{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce5e2ab",
   "metadata": {},
   "source": [
    "# Binary Segmentation Inference Pipeline for Path Detection\n",
    "\n",
    "## Overview\n",
    "This notebook implements an advanced path detection system that combines traditional segmentation with state-of-the-art refinement techniques for robust path identification in peatland environments.\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### 1. Binary U-Net Segmentation\n",
    "- **Purpose**: Primary path detection\n",
    "- **Features**:\n",
    "  * Real-time capable processing\n",
    "  * Temporal smoothing integration\n",
    "  * Robust initial predictions\n",
    "- **Output**: Coarse path masks\n",
    "\n",
    "### 2. Segment Anything Model (SAM) Refinement\n",
    "- **Purpose**: High-precision boundary refinement\n",
    "- **Features**:\n",
    "  * Zero-shot adaptation capability\n",
    "  * Point-prompt based refinement\n",
    "  * Multi-mask prediction\n",
    "- **Output**: Fine-grained path boundaries\n",
    "\n",
    "## Pipeline Integration\n",
    "\n",
    "### Processing Flow\n",
    "1. Initial path detection via U-Net\n",
    "2. Temporal consistency through weighted averaging\n",
    "3. SAM-based boundary refinement\n",
    "4. Visualization with semi-transparent overlays\n",
    "\n",
    "### Key Features\n",
    "- Robust handling of challenging scenes\n",
    "- Adaptive prompt generation\n",
    "- Real-time processing capability\n",
    "- Smooth temporal transitions\n",
    "\n",
    "This system provides reliable path detection for autonomous navigation in complex peatland environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e311aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2 # OpenCV for video processing\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9ce85",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "1. **Binary U-Net Settings**\n",
    "   - **Model Selection**:\n",
    "     * Run-specific checkpoint loading\n",
    "     * Performance-optimized weights\n",
    "     * Architecture configuration\n",
    "   \n",
    "   - **Processing Parameters**:\n",
    "     * Input resolution\n",
    "     * Batch processing\n",
    "     * Memory optimization\n",
    "\n",
    "2. **SAM Configuration**\n",
    "   - **Model Variant**:\n",
    "     * ViT-B architecture selection\n",
    "     * Checkpoint management\n",
    "     * Resource allocation\n",
    "   \n",
    "   - **Runtime Settings**:\n",
    "     * Inference parameters\n",
    "     * Multi-mask generation\n",
    "     * Point prompt configuration\n",
    "\n",
    "### Processing Pipeline Configuration\n",
    "\n",
    "1. **Video Input Settings**\n",
    "   - **Source Management**:\n",
    "     * Video clip selection\n",
    "     * Frame rate handling\n",
    "     * Resolution configuration\n",
    "   \n",
    "   - **Format Handling**:\n",
    "     * Codec compatibility\n",
    "     * Color space management\n",
    "     * Memory efficiency\n",
    "\n",
    "2. **Output Organization**\n",
    "   - **Directory Structure**:\n",
    "     * Run-specific organization\n",
    "     * Consistent naming conventions\n",
    "     * Version control support\n",
    "   \n",
    "   - **File Management**:\n",
    "     * Automatic path generation\n",
    "     * Existing file handling\n",
    "     * Space optimization\n",
    "\n",
    "### Hardware Optimization\n",
    "- Automatic device selection (CUDA/MPS/CPU)\n",
    "- Memory usage optimization\n",
    "- Processing speed adaptation\n",
    "- Resource allocation\n",
    "\n",
    "These configurations ensure:\n",
    "- Consistent processing\n",
    "- Reproducible results\n",
    "- Efficient resource utilization\n",
    "- Organized output management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d6f8240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from run: binary_unet_2025-08-03_22-53-32\n",
      "Using SAM checkpoint: metrics/sam_vit_b.pth\n",
      "Processing video: ../data/video/splits/Clip_1_42s.mp4\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Specify the name of the binary U-Net training run you want to use.\n",
    "RUN_NAME = \"binary_unet_2025-08-03_22-53-32\" # <--- CHANGE THIS\n",
    "\n",
    "# --- SAM Model Configuration ---\n",
    "SAM_CHECKPOINT_PATH = Path(\"./metrics/sam_vit_b.pth\") # <--- Path to your downloaded SAM model\n",
    "SAM_MODEL_TYPE = \"vit_b\"\n",
    "\n",
    "# Specify which video clip to process\n",
    "VIDEO_FILENAME = \"Clip_1_42s.mp4\" # <--- CHANGE THIS\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME\n",
    "MODEL_PATH = METRICS_DIR / \"best_binary_model.pth\"\n",
    "VIDEO_PATH = Path(\"../data/video/splits\") / VIDEO_FILENAME\n",
    "OUTPUT_DIR = METRICS_DIR / \"video_predictions\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / f\"predicted_binary_{VIDEO_FILENAME}\"\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Loading model from run: {RUN_NAME}\")\n",
    "print(f\"Using SAM checkpoint: {SAM_CHECKPOINT_PATH}\")\n",
    "print(f\"Processing video: {VIDEO_PATH}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb052ea",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Preprocessing Pipeline\n",
    "\n",
    "### Image Processing Framework\n",
    "\n",
    "1. **Input Standardization**\n",
    "   - **Resolution Control**:\n",
    "     * Fixed dimensions (480x640)\n",
    "     * Aspect ratio management\n",
    "     * Scale normalization\n",
    "   \n",
    "   - **Color Processing**:\n",
    "     * RGB channel handling\n",
    "     * ImageNet normalization\n",
    "     * Intensity scaling\n",
    "\n",
    "2. **Augmentation Pipeline**\n",
    "   - **Albumentations Integration**:\n",
    "     * Efficient transformation pipeline\n",
    "     * GPU-compatible operations\n",
    "     * Memory-optimized processing\n",
    "\n",
    "### Model Architecture Setup\n",
    "\n",
    "1. **Binary U-Net Configuration**\n",
    "   - **Architecture Details**:\n",
    "     * ResNet34 backbone selection\n",
    "     * Skip connection implementation\n",
    "     * Binary classification head\n",
    "   \n",
    "   - **Initialization Process**:\n",
    "     * Weight loading verification\n",
    "     * Device optimization\n",
    "     * Memory allocation\n",
    "\n",
    "2. **SAM Integration**\n",
    "   - **Model Setup**:\n",
    "     * ViT-B architecture loading\n",
    "     * Checkpoint verification\n",
    "     * Device placement\n",
    "   \n",
    "   - **Predictor Configuration**:\n",
    "     * Point prompt interface setup\n",
    "     * Multi-mask generation\n",
    "     * Zero-shot adaptation\n",
    "\n",
    "### System Integration\n",
    "- Coordinated model initialization\n",
    "- Memory-efficient loading\n",
    "- Device-specific optimization\n",
    "- Pipeline synchronization\n",
    "\n",
    "This setup ensures efficient and reliable model deployment for real-time path detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c96ec06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net model loaded successfully.\n",
      "SAM model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# We only need the basic transforms for inference\n",
    "unet_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "unet_model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=2).to(DEVICE)\n",
    "unet_model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "unet_model.eval()\n",
    "print(\"U-Net model loaded successfully.\")\n",
    "\n",
    "sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "print(\"SAM model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb6e63",
   "metadata": {},
   "source": [
    "## 4. Core Processing Functions\n",
    "\n",
    "### Path Detection Pipeline Components\n",
    "\n",
    "1. **Smoothed U-Net Prediction System**\n",
    "   \n",
    "   **Function**: `get_smoothed_unet_prediction`\n",
    "   - **Input Processing**:\n",
    "     * RGB frame conversion\n",
    "     * Resolution standardization\n",
    "     * Tensor preparation\n",
    "   \n",
    "   - **Prediction Pipeline**:\n",
    "     * U-Net inference\n",
    "     * Temporal smoothing\n",
    "     * Weighted averaging\n",
    "   \n",
    "   - **Output Processing**:\n",
    "     * Resolution restoration\n",
    "     * Binary mask generation\n",
    "     * Memory optimization\n",
    "\n",
    "2. **Prompt Generation Framework**\n",
    "   \n",
    "   **Function**: `mask_to_prompt_points`\n",
    "   - **Contour Analysis**:\n",
    "     * Path region identification\n",
    "     * Area-based filtering\n",
    "     * Multi-point selection\n",
    "   \n",
    "   - **Point Generation**:\n",
    "     * Centroid calculation\n",
    "     * Multiple prompt support\n",
    "     * Spatial distribution\n",
    "\n",
    "### Advanced Features\n",
    "\n",
    "1. **Temporal Consistency**\n",
    "   - **Moving Average Implementation**:\n",
    "     * Weight distribution optimization\n",
    "     * Historical context integration\n",
    "     * Smooth transition management\n",
    "   \n",
    "   - **State Management**:\n",
    "     * History buffer maintenance\n",
    "     * Update frequency control\n",
    "     * Memory efficiency\n",
    "\n",
    "2. **Robustness Mechanisms**\n",
    "   - **Fallback Strategies**:\n",
    "     * No-detection handling\n",
    "     * State persistence\n",
    "     * Recovery procedures\n",
    "   \n",
    "   - **Quality Assurance**:\n",
    "     * Contour validation\n",
    "     * Area thresholding\n",
    "     * Consistency checking\n",
    "\n",
    "### System Integration\n",
    "- Efficient memory utilization\n",
    "- Real-time processing capability\n",
    "- Error handling mechanisms\n",
    "- Pipeline synchronization\n",
    "\n",
    "These functions form the core processing engine for reliable path detection and refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b68fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COLOR_BGR = (152, 16, 60) # Purple\n",
    "PATH_CLASS_ID = 1\n",
    "\n",
    "def get_smoothed_unet_prediction(frame: np.ndarray, logits_history: deque):\n",
    "    \"\"\"\n",
    "    Gets a U-Net prediction and applies a weighted moving average over the\n",
    "    last N frames for temporal smoothing.\n",
    "    \"\"\"\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    augmented = unet_transform(image=image_rgb)\n",
    "    image_tensor = augmented['image'].unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        current_logits = unet_model(image_tensor)\n",
    "    \n",
    "    # Add the latest prediction to our history\n",
    "    logits_history.append(current_logits)\n",
    "    \n",
    "    # --- Weighted Moving Average ---\n",
    "    # Create weights that give more importance to recent frames\n",
    "    weights = torch.linspace(0.1, 1.0, len(logits_history)).to(DEVICE)\n",
    "    weights = weights / weights.sum() # Normalize weights to sum to 1\n",
    "    \n",
    "    # Calculate the weighted average of the logits in the history\n",
    "    # Reshape weights to match the tensor dimensions for broadcasting\n",
    "    weighted_sum = torch.zeros_like(current_logits)\n",
    "    for i, logits in enumerate(logits_history):\n",
    "        weighted_sum += logits * weights[i]\n",
    "        \n",
    "    preds = torch.argmax(weighted_sum, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    final_mask = cv2.resize(preds, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return final_mask\n",
    "\n",
    "def mask_to_prompt_points(mask: np.ndarray, max_points: 3):\n",
    "    \"\"\"\n",
    "    Finds the largest contour in the U-Net mask and returns its center point.\n",
    "    This point will be the prompt for SAM.\n",
    "    \"\"\"\n",
    "    # Find all contours of the path\n",
    "    contours, _ = cv2.findContours((mask == PATH_CLASS_ID).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        return None # No path found by U-Net\n",
    "        \n",
    "    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    points = []\n",
    "    for contour in sorted_contours[:max_points]:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "            center_y = int(M[\"m01\"] / M[\"m00\"])\n",
    "            points.append([center_x, center_y])\n",
    "            \n",
    "    return np.array(points) if points else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5ef09",
   "metadata": {},
   "source": [
    "## 5. Video Processing Pipeline\n",
    "\n",
    "### Processing Framework\n",
    "\n",
    "1. **Frame Acquisition and Management**\n",
    "   - **Video Stream Handling**:\n",
    "     * Frame extraction\n",
    "     * Format verification\n",
    "     * Resolution management\n",
    "     * Memory optimization\n",
    "   \n",
    "   - **State Management**:\n",
    "     * Buffer initialization\n",
    "     * History tracking\n",
    "     * Resource allocation\n",
    "\n",
    "2. **Path Detection Workflow**\n",
    "\n",
    "   **Initial Detection Phase**:\n",
    "   - U-Net prediction generation\n",
    "   - Temporal smoothing application\n",
    "   - Confidence validation\n",
    "   - State persistence\n",
    "   \n",
    "   **Refinement Phase**:\n",
    "   - Point prompt extraction\n",
    "   - SAM model adaptation\n",
    "   - Multi-mask generation\n",
    "   - IoU-based selection\n",
    "\n",
    "3. **Robustness Mechanisms**\n",
    "\n",
    "   **State Handling**:\n",
    "   - Path point tracking\n",
    "   - Historical context\n",
    "   - Recovery procedures\n",
    "   - Fallback strategies\n",
    "   \n",
    "   **Quality Assurance**:\n",
    "   - Prediction validation\n",
    "   - Consistency checking\n",
    "   - Error recovery\n",
    "   - Performance monitoring\n",
    "\n",
    "4. **Visualization Pipeline**\n",
    "\n",
    "   **Mask Rendering**:\n",
    "   - Semi-transparent overlay\n",
    "   - Color scheme application\n",
    "   - Path highlighting\n",
    "   - Visual clarity\n",
    "   \n",
    "   **Frame Composition**:\n",
    "   - Layer blending\n",
    "   - Opacity control\n",
    "   - Visual feedback\n",
    "   - Quality assurance\n",
    "\n",
    "### System Integration\n",
    "- Efficient frame processing\n",
    "- Memory management\n",
    "- Error handling\n",
    "- Performance optimization\n",
    "\n",
    "This comprehensive pipeline ensures:\n",
    "- Reliable path detection\n",
    "- Smooth visual output\n",
    "- Real-time processing\n",
    "- Robust performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cf1033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1287 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [11:56<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete.\n",
      "Output video saved to: metrics/binary_unet_2025-08-03_22-53-32/video_predictions/predicted_binary_Clip_1_42s.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {VIDEO_PATH}\")\n",
    "else:\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(OUTPUT_VIDEO_PATH), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing {total_frames} frames...\")\n",
    "\n",
    "    last_known_points = None\n",
    "    logits_history = deque(maxlen=10) \n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # 1. Get coarse prediction from U-Net\n",
    "        unet_mask = get_smoothed_unet_prediction(frame, logits_history)\n",
    "        \n",
    "        # 2. Convert mask into multiple point prompts\n",
    "        current_points = mask_to_prompt_points(unet_mask, max_points=3)\n",
    "        \n",
    "        # 3. Fallback logic and state update\n",
    "        prompt_points_for_sam = None\n",
    "        if current_points is not None:\n",
    "            prompt_points_for_sam = current_points\n",
    "            last_known_points = current_points # Update our state\n",
    "        elif last_known_points is not None:\n",
    "            prompt_points_for_sam = last_known_points # Use the fallback\n",
    "        \n",
    "        final_mask = np.zeros_like(unet_mask, dtype=bool)\n",
    "\n",
    "        # 4. If we have any points to use, run SAM\n",
    "        if prompt_points_for_sam is not None:\n",
    "            sam_predictor.set_image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Create a label for each point (all are foreground prompts)\n",
    "            input_labels = np.ones(len(prompt_points_for_sam))\n",
    "            \n",
    "            masks_sam, scores, _ = sam_predictor.predict(\n",
    "                point_coords=prompt_points_for_sam,\n",
    "                point_labels=input_labels,\n",
    "                multimask_output=True,\n",
    "            )\n",
    "\n",
    "            # Select the best mask based on IoU with the U-Net prediction\n",
    "            best_iou = -1\n",
    "            best_mask_idx = -1\n",
    "            unet_path_mask = (unet_mask == PATH_CLASS_ID)\n",
    "\n",
    "            for i, mask in enumerate(masks_sam):\n",
    "                intersection = np.logical_and(unet_path_mask, mask).sum()\n",
    "                union = np.logical_or(unet_path_mask, mask).sum()\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                \n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_mask_idx = i\n",
    "            \n",
    "            if best_mask_idx != -1:\n",
    "                final_mask = masks_sam[best_mask_idx]\n",
    "\n",
    "        # 5. Create the visualization\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        overlay[final_mask] = PATH_COLOR_BGR\n",
    "        final_frame = cv2.addWeighted(frame, 1.0, overlay, 0.6, 0)\n",
    "        \n",
    "        out.write(final_frame)\n",
    "\n",
    "        # 4. Create the visualization\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        overlay[final_mask] = PATH_COLOR_BGR\n",
    "        final_frame = cv2.addWeighted(frame, 1.0, overlay, 0.6, 0)\n",
    "        \n",
    "        out.write(final_frame)\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
