{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce5e2ab",
   "metadata": {},
   "source": [
    "# Binary Segmentation Inference with SAM Refinement\n",
    "\n",
    "This notebook implements an advanced path detection pipeline combining:\n",
    "\n",
    "1. **Binary U-Net Segmentation**:\n",
    "   - Initial path detection\n",
    "   - Temporally smoothed predictions\n",
    "   - Fast, real-time capable processing\n",
    "\n",
    "2. **Segment Anything Model (SAM)**:\n",
    "   - Refinement of U-Net predictions\n",
    "   - High-precision boundary detection\n",
    "   - Point-prompt based segmentation\n",
    "\n",
    "The pipeline processes video input and generates path annotations with:\n",
    "- Temporal consistency through weighted averaging\n",
    "- Robust handling of challenging frames\n",
    "- Semi-transparent path overlays for visualization\n",
    "\n",
    "## 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e311aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2 # OpenCV for video processing\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9ce85",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Key settings for the inference pipeline:\n",
    "\n",
    "### Model Settings\n",
    "- **Binary U-Net**: Trained model selection via run name\n",
    "- **SAM Model**: Pre-trained ViT-B checkpoint configuration\n",
    "- **Device**: Automatic hardware acceleration selection\n",
    "\n",
    "### Video Processing\n",
    "- Input video selection from splits directory\n",
    "- Output directory organization by run name\n",
    "- Automatic file path handling\n",
    "\n",
    "### Output Configuration\n",
    "- Structured directory creation\n",
    "- Consistent naming conventions\n",
    "- MP4 format video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d6f8240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from run: binary_unet_2025-08-03_22-53-32\n",
      "Using SAM checkpoint: metrics/sam_vit_b.pth\n",
      "Processing video: ../data/video/splits/Clip_1_42s.mp4\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Specify the name of the binary U-Net training run you want to use.\n",
    "RUN_NAME = \"binary_unet_2025-08-03_22-53-32\" # <--- CHANGE THIS\n",
    "\n",
    "# --- SAM Model Configuration ---\n",
    "SAM_CHECKPOINT_PATH = Path(\"./metrics/sam_vit_b.pth\") # <--- Path to your downloaded SAM model\n",
    "SAM_MODEL_TYPE = \"vit_b\"\n",
    "\n",
    "# Specify which video clip to process\n",
    "VIDEO_FILENAME = \"Clip_1_42s.mp4\" # <--- CHANGE THIS\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME\n",
    "MODEL_PATH = METRICS_DIR / \"best_binary_model.pth\"\n",
    "VIDEO_PATH = Path(\"../data/video/splits\") / VIDEO_FILENAME\n",
    "OUTPUT_DIR = METRICS_DIR / \"video_predictions\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / f\"predicted_binary_{VIDEO_FILENAME}\"\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Loading model from run: {RUN_NAME}\")\n",
    "print(f\"Using SAM checkpoint: {SAM_CHECKPOINT_PATH}\")\n",
    "print(f\"Processing video: {VIDEO_PATH}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb052ea",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Preprocessing\n",
    "\n",
    "### Image Processing Pipeline\n",
    "- Fixed input dimensions (480x640)\n",
    "- ImageNet normalization\n",
    "- Albumentations-based transformations\n",
    "\n",
    "### Model Initialization\n",
    "1. **Binary U-Net**:\n",
    "   - ResNet34 backbone\n",
    "   - Two-class segmentation\n",
    "   - CPU/GPU compatibility\n",
    "\n",
    "2. **SAM Model**:\n",
    "   - ViT-B architecture\n",
    "   - Zero-shot capability\n",
    "   - Point prompt interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c96ec06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net model loaded successfully.\n",
      "SAM model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# We only need the basic transforms for inference\n",
    "unet_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "unet_model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=2).to(DEVICE)\n",
    "unet_model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "unet_model.eval()\n",
    "print(\"U-Net model loaded successfully.\")\n",
    "\n",
    "sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "print(\"SAM model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb6e63",
   "metadata": {},
   "source": [
    "## 4. Utility Functions\n",
    "\n",
    "### Path Detection Pipeline\n",
    "\n",
    "1. **Smoothed U-Net Prediction** (`get_smoothed_unet_prediction`):\n",
    "   - Processes RGB frames through U-Net\n",
    "   - Implements temporal smoothing via weighted averaging\n",
    "   - Handles resolution matching\n",
    "\n",
    "2. **Prompt Generation** (`mask_to_prompt_points`):\n",
    "   - Extracts path center points from U-Net masks\n",
    "   - Supports multiple point prompts\n",
    "   - Provides fallback mechanisms for robustness\n",
    "\n",
    "### Key Features\n",
    "- Temporal consistency through moving average\n",
    "- Adaptive prompt generation\n",
    "- Robust contour analysis\n",
    "- Automatic resolution handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b68fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COLOR_BGR = (152, 16, 60) # Purple\n",
    "PATH_CLASS_ID = 1\n",
    "\n",
    "def get_smoothed_unet_prediction(frame: np.ndarray, logits_history: deque):\n",
    "    \"\"\"\n",
    "    Gets a U-Net prediction and applies a weighted moving average over the\n",
    "    last N frames for temporal smoothing.\n",
    "    \"\"\"\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    augmented = unet_transform(image=image_rgb)\n",
    "    image_tensor = augmented['image'].unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        current_logits = unet_model(image_tensor)\n",
    "    \n",
    "    # Add the latest prediction to our history\n",
    "    logits_history.append(current_logits)\n",
    "    \n",
    "    # --- Weighted Moving Average ---\n",
    "    # Create weights that give more importance to recent frames\n",
    "    weights = torch.linspace(0.1, 1.0, len(logits_history)).to(DEVICE)\n",
    "    weights = weights / weights.sum() # Normalize weights to sum to 1\n",
    "    \n",
    "    # Calculate the weighted average of the logits in the history\n",
    "    # Reshape weights to match the tensor dimensions for broadcasting\n",
    "    weighted_sum = torch.zeros_like(current_logits)\n",
    "    for i, logits in enumerate(logits_history):\n",
    "        weighted_sum += logits * weights[i]\n",
    "        \n",
    "    preds = torch.argmax(weighted_sum, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    final_mask = cv2.resize(preds, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return final_mask\n",
    "\n",
    "def mask_to_prompt_points(mask: np.ndarray, max_points: 3):\n",
    "    \"\"\"\n",
    "    Finds the largest contour in the U-Net mask and returns its center point.\n",
    "    This point will be the prompt for SAM.\n",
    "    \"\"\"\n",
    "    # Find all contours of the path\n",
    "    contours, _ = cv2.findContours((mask == PATH_CLASS_ID).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        return None # No path found by U-Net\n",
    "        \n",
    "    sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    points = []\n",
    "    for contour in sorted_contours[:max_points]:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "            center_y = int(M[\"m01\"] / M[\"m00\"])\n",
    "            points.append([center_x, center_y])\n",
    "            \n",
    "    return np.array(points) if points else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5ef09",
   "metadata": {},
   "source": [
    "## 5. Video Processing Pipeline\n",
    "\n",
    "The main processing loop implements a sophisticated path detection workflow:\n",
    "\n",
    "### Frame Processing Steps\n",
    "1. **Initial Detection**:\n",
    "   - U-Net prediction\n",
    "   - Temporal smoothing application\n",
    "   \n",
    "2. **SAM Refinement**:\n",
    "   - Point prompt generation\n",
    "   - Multiple mask prediction\n",
    "   - Best mask selection via IoU\n",
    "\n",
    "3. **State Management**:\n",
    "   - Path point tracking\n",
    "   - Fallback handling\n",
    "   - Temporal consistency\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Semi-transparent overlays\n",
    "   - Path highlighting\n",
    "   - Frame composition\n",
    "\n",
    "The pipeline maintains smooth transitions and robust detection across challenging frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cf1033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1287 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [11:56<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete.\n",
      "Output video saved to: metrics/binary_unet_2025-08-03_22-53-32/video_predictions/predicted_binary_Clip_1_42s.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {VIDEO_PATH}\")\n",
    "else:\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(OUTPUT_VIDEO_PATH), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing {total_frames} frames...\")\n",
    "\n",
    "    last_known_points = None\n",
    "    logits_history = deque(maxlen=10) \n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # 1. Get coarse prediction from U-Net\n",
    "        unet_mask = get_smoothed_unet_prediction(frame, logits_history)\n",
    "        \n",
    "        # 2. Convert mask into multiple point prompts\n",
    "        current_points = mask_to_prompt_points(unet_mask, max_points=3)\n",
    "        \n",
    "        # 3. Fallback logic and state update\n",
    "        prompt_points_for_sam = None\n",
    "        if current_points is not None:\n",
    "            prompt_points_for_sam = current_points\n",
    "            last_known_points = current_points # Update our state\n",
    "        elif last_known_points is not None:\n",
    "            prompt_points_for_sam = last_known_points # Use the fallback\n",
    "        \n",
    "        final_mask = np.zeros_like(unet_mask, dtype=bool)\n",
    "\n",
    "        # 4. If we have any points to use, run SAM\n",
    "        if prompt_points_for_sam is not None:\n",
    "            sam_predictor.set_image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Create a label for each point (all are foreground prompts)\n",
    "            input_labels = np.ones(len(prompt_points_for_sam))\n",
    "            \n",
    "            masks_sam, scores, _ = sam_predictor.predict(\n",
    "                point_coords=prompt_points_for_sam,\n",
    "                point_labels=input_labels,\n",
    "                multimask_output=True,\n",
    "            )\n",
    "\n",
    "            # Select the best mask based on IoU with the U-Net prediction\n",
    "            best_iou = -1\n",
    "            best_mask_idx = -1\n",
    "            unet_path_mask = (unet_mask == PATH_CLASS_ID)\n",
    "\n",
    "            for i, mask in enumerate(masks_sam):\n",
    "                intersection = np.logical_and(unet_path_mask, mask).sum()\n",
    "                union = np.logical_or(unet_path_mask, mask).sum()\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                \n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_mask_idx = i\n",
    "            \n",
    "            if best_mask_idx != -1:\n",
    "                final_mask = masks_sam[best_mask_idx]\n",
    "\n",
    "        # 5. Create the visualization\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        overlay[final_mask] = PATH_COLOR_BGR\n",
    "        final_frame = cv2.addWeighted(frame, 1.0, overlay, 0.6, 0)\n",
    "        \n",
    "        out.write(final_frame)\n",
    "\n",
    "        # 4. Create the visualization\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        overlay[final_mask] = PATH_COLOR_BGR\n",
    "        final_frame = cv2.addWeighted(frame, 1.0, overlay, 0.6, 0)\n",
    "        \n",
    "        out.write(final_frame)\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
