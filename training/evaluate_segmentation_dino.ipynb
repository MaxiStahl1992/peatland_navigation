{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f4114a",
   "metadata": {},
   "source": [
    "# DinoV2-Based Semantic Segmentation Evaluation\n",
    "\n",
    "This notebook evaluates the performance of a semantic segmentation model based on Facebook's DinoV2 architecture. The model performs multi-class segmentation of peatland imagery, distinguishing between:\n",
    "\n",
    "- Paths\n",
    "- Natural Ground\n",
    "- Trees\n",
    "- Vegetation\n",
    "- Ignore (regions not considered in evaluation)\n",
    "\n",
    "The evaluation process includes:\n",
    "- Quantitative metrics (IoU, pixel accuracy)\n",
    "- Per-class performance analysis\n",
    "- Visual comparison of predictions\n",
    "- Comprehensive result logging\n",
    "\n",
    "DinoV2's self-supervised pre-training enables effective feature extraction for the semantic segmentation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14e1a7",
   "metadata": {},
   "source": [
    "## 1. Required Libraries\n",
    "\n",
    "Key dependencies for the evaluation:\n",
    "\n",
    "- **torch**: Deep learning framework\n",
    "- **segmentation_models_pytorch**: Semantic segmentation utilities\n",
    "- **transformers**: Access to DinoV2 model and processor\n",
    "- **albumentations**: Image augmentation and transformation\n",
    "- **torchmetrics**: Evaluation metrics (JaccardIndex)\n",
    "- **PIL, numpy, pandas**: Data handling and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e6240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "from transformers import Dinov2Model, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcffe3",
   "metadata": {},
   "source": [
    "## 2. Dataset and Model Architecture\n",
    "\n",
    "### Dataset Implementation\n",
    "The `PeatlandDinoDataset` class provides:\n",
    "- Custom data loading for DinoV2 compatibility\n",
    "- Integrated image processing using DinoV2's processor\n",
    "- Support for image transformations\n",
    "- Synchronized image-mask pair handling\n",
    "\n",
    "### Model Architecture\n",
    "The `DinoV2ForSemanticSegmentation` implements:\n",
    "- Frozen DinoV2-base backbone for feature extraction\n",
    "- Custom decoder head for upsampling\n",
    "- Multi-scale feature processing\n",
    "- Output adaptation for 5-class segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5630cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef9c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeatlandDinoDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset adapted for DinoV2's image processor.\"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, image_processor, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=mask)\n",
    "            image = Image.fromarray(augmented['image'])\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return pixel_values, mask\n",
    "\n",
    "class DinoV2ForSemanticSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(DinoV2ForSemanticSegmentation, self).__init__()\n",
    "        self.dinov2 = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.dinov2(pixel_values, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        patch_tokens = last_hidden_state[:, 1:, :]\n",
    "        batch_size, seq_len, num_channels = patch_tokens.shape\n",
    "        height = width = int(seq_len**0.5)\n",
    "        feature_map = patch_tokens.permute(0, 2, 1).contiguous().reshape(batch_size, num_channels, height, width)\n",
    "        logits = self.head(feature_map)\n",
    "        final_logits = nn.functional.interpolate(logits, size=(DINO_IMAGE_SIZE, DINO_IMAGE_SIZE), mode='bilinear', align_corners=False)\n",
    "        return final_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de344",
   "metadata": {},
   "source": [
    "## 3. Data Processing Setup\n",
    "\n",
    "The evaluation pipeline uses two key components:\n",
    "\n",
    "1. **DinoV2 Image Processor**:\n",
    "   - Handles DinoV2-specific image preprocessing\n",
    "   - Ensures input format compatibility\n",
    "   - Maintains consistent image normalization\n",
    "\n",
    "2. **Evaluation Transforms**:\n",
    "   - Resizes images to DinoV2's required input size (224x224)\n",
    "   - Preserves aspect ratio\n",
    "   - Maintains spatial correspondence between images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c7b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "eval_transform = A.Compose([A.Resize(height=DINO_IMAGE_SIZE, width=DINO_IMAGE_SIZE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97766aa1",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from run: dinov2_2025-08-03_17-48-54\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Specify the training run to evaluate\n",
    "RUN_NAME = \"dinov2_2025-08-03_17-48-54\"  # Update this for each evaluation\n",
    "\n",
    "# Set up paths and directories\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME                 # Directory for storing results\n",
    "MODEL_PATH = METRICS_DIR / \"best_model.pth\"             # Path to saved model weights\n",
    "BATCH_SIZE = 4                                          # Evaluation batch size\n",
    "\n",
    "# Determine compute device\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"           # Use GPU if available\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"  # Use Apple Silicon if available\n",
    "else: DEVICE = \"cpu\"                                    # Fall back to CPU\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Evaluating model from run: {RUN_NAME}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9666182",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426eb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 204 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "BASE_PROCESSED_DIR = Path(\"../data/processed/segmentation\")\n",
    "TEST_IMG_DIR = BASE_PROCESSED_DIR / \"test\" / \"images\"\n",
    "TEST_MASK_DIR = BASE_PROCESSED_DIR / \"test\" / \"masks\"\n",
    "\n",
    "test_dataset = PeatlandDinoDataset(\n",
    "    images_dir=TEST_IMG_DIR,\n",
    "    masks_dir=TEST_MASK_DIR,\n",
    "    image_processor=processor,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Loaded {len(test_dataset)} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8241d",
   "metadata": {},
   "source": [
    "## 6. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b16e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = DinoV2ForSemanticSegmentation(num_classes=5).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deae95",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "The evaluation process computes comprehensive metrics:\n",
    "\n",
    "1. **Per-Class IoU (Intersection over Union)**:\n",
    "   - Calculated for each semantic class\n",
    "   - Handles class imbalance\n",
    "   - Identifies per-class model strengths/weaknesses\n",
    "\n",
    "2. **Mean IoU (mIoU)**:\n",
    "   - Average IoU across all classes\n",
    "   - Key metric for overall model performance\n",
    "\n",
    "3. **Pixel Accuracy**:\n",
    "   - Percentage of correctly classified pixels\n",
    "   - Provides intuitive model performance measure\n",
    "\n",
    "Classes evaluated:\n",
    "- PATH: Traversable pathways\n",
    "- NATURAL_GROUND: Bare earth/soil\n",
    "- TREE: Tree coverage\n",
    "- VEGETATION: Non-tree vegetation\n",
    "- IGNORE: Regions excluded from evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04300b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████| 51/51 [00:20<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Pixel Accuracy: 86.29%\n",
      "Mean IoU (mIoU): 0.6565\n",
      "\n",
      "IoU per Class:\n",
      "  - PATH: 0.7115\n",
      "  - NATURAL_GROUND: 0.7868\n",
      "  - TREE: 0.2320\n",
      "  - VEGETATION: 0.7450\n",
      "  - IGNORE: 0.8070\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "CLASS_NAMES = [\"PATH\", \"NATURAL_GROUND\", \"TREE\", \"VEGETATION\", \"IGNORE\"]\n",
    "jaccard = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(DEVICE)\n",
    "total_correct_pixels, total_pixels = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        jaccard.update(preds, masks)\n",
    "        total_correct_pixels += (preds == masks).sum().item()\n",
    "        total_pixels += torch.numel(masks)\n",
    "\n",
    "pixel_accuracy = (total_correct_pixels / total_pixels) * 100\n",
    "iou_per_class = jaccard.compute()\n",
    "mean_iou = iou_per_class.mean()\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Overall Pixel Accuracy: {pixel_accuracy:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {mean_iou:.4f}\")\n",
    "print(\"\\nIoU per Class:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  - {class_name}: {iou_per_class[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22583c",
   "metadata": {},
   "source": [
    "## 8. Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b798271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics saved to: metrics/dinov2_2025-08-03_17-48-54/test_set_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "    'Metric': ['Pixel Accuracy', 'Mean IoU'] + [f'IoU_{name}' for name in CLASS_NAMES],\n",
    "    'Value': [pixel_accuracy, mean_iou.item()] + iou_per_class.cpu().numpy().tolist()\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "output_csv_path = METRICS_DIR / \"test_set_evaluation.csv\"\n",
    "metrics_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nEvaluation metrics saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9bb16",
   "metadata": {},
   "source": [
    "## 9. Visualization\n",
    "\n",
    "The visualization process generates comparison plots for qualitative analysis:\n",
    "\n",
    "1. **Original Image**: \n",
    "   - Input image at full resolution\n",
    "   - Maintains original aspect ratio and details\n",
    "\n",
    "2. **Ground Truth Mask**:\n",
    "   - Color-coded semantic labels\n",
    "   - PATH: Purple\n",
    "   - NATURAL_GROUND: Blue\n",
    "   - TREE: Light Blue\n",
    "   - VEGETATION: Yellow\n",
    "   - IGNORE: Black\n",
    "\n",
    "3. **Model Predictions**:\n",
    "   - Uses same color scheme as ground truth\n",
    "   - Enables direct visual comparison\n",
    "   - Helps identify systematic errors\n",
    "\n",
    "Visualizations are saved as high-resolution PNG files for detailed inspection and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54ecd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 visualization samples to: metrics/dinov2_2025-08-03_17-48-54/visualizations\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset, model, device, num_samples=5):\n",
    "    vis_dir = METRICS_DIR / \"visualizations\"\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    color_map = np.array([\n",
    "        [60, 16, 152], [132, 41, 246], [110, 193, 228],\n",
    "        [254, 221, 58], [0, 0, 0]\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get processed data for the model\n",
    "            image_tensor, gt_mask = dataset[i]\n",
    "            \n",
    "            # Load the original, unprocessed image for clean visualization\n",
    "            original_image_path = dataset.images_dir / dataset.image_filenames[i]\n",
    "            display_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Get prediction\n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Apply color map to masks\n",
    "            gt_mask_color = color_map[gt_mask.cpu().numpy()]\n",
    "            pred_mask_color = color_map[pred_mask]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "            fig.suptitle(f\"Sample {i}\", fontsize=16)\n",
    "            \n",
    "            ax[0].imshow(display_image)\n",
    "            ax[0].set_title(\"Original Image\")\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(gt_mask_color)\n",
    "            ax[1].set_title(\"Ground Truth Mask\")\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            ax[2].imshow(pred_mask_color)\n",
    "            ax[2].set_title(\"Model Prediction\")\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Saved {num_samples} visualization samples to: {vis_dir}\")\n",
    "\n",
    "visualize_predictions(test_dataset, model, DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
