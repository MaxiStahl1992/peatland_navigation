{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606024f3",
   "metadata": {},
   "source": [
    "# Multi-Class Semantic Segmentation Training\n",
    "\n",
    "This notebook implements semantic segmentation for peatland navigation using a U-Net architecture with a ResNet34 backbone. The model performs pixel-wise classification of peatland terrain features for safe and efficient navigation.\n",
    "\n",
    "Target Classes:\n",
    "1. Path (Primary navigation routes)\n",
    "2. Natural Ground (Safe traversable terrain)\n",
    "3. Tree (Obstacles and landmarks)\n",
    "4. Vegetation (Secondary terrain features)\n",
    "5. Ignore/Background (Areas outside annotation scope)\n",
    "\n",
    "Key Features:\n",
    "- ResNet34 encoder with ImageNet pretraining\n",
    "- U-Net decoder for precise segmentation\n",
    "- Efficient data augmentation pipeline\n",
    "- Multi-class pixel-wise classification\n",
    "- Comprehensive metric tracking\n",
    "\n",
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Deep learning and numerical computing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Computer vision and image processing\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Data handling and utilities\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958230ea",
   "metadata": {},
   "source": [
    "## 2. Dataset Implementation\n",
    "\n",
    "The custom `PeatlandDataset` class provides:\n",
    "\n",
    "1. Data Loading:\n",
    "   - RGB image loading with format consistency\n",
    "   - Multi-class segmentation mask handling\n",
    "   - Automatic path resolution and file matching\n",
    "\n",
    "2. Data Processing:\n",
    "   - Dynamic transformation pipeline\n",
    "   - Albumentations integration for augmentation\n",
    "   - Proper tensor type conversion\n",
    "\n",
    "3. Memory Efficiency:\n",
    "   - On-demand data loading\n",
    "   - Efficient file handling\n",
    "   - Optimized memory usage\n",
    "\n",
    "The implementation ensures proper alignment between images and their corresponding segmentation masks throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeatlandDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for peatland semantic segmentation.\n",
    "    \n",
    "    This dataset handles the loading and processing of peatland images and their\n",
    "    corresponding segmentation masks for multi-class terrain classification.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str or Path): Directory containing input RGB images\n",
    "        masks_dir (str or Path): Directory containing segmentation masks\n",
    "        transform (callable, optional): Transformations to apply to image-mask pairs\n",
    "        \n",
    "    The dataset expects:\n",
    "    1. Matching filenames between images and masks\n",
    "    2. RGB images in standard formats (jpg, png)\n",
    "    3. Single-channel integer masks with class labels\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        # Convert paths to Path objects for consistent handling\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Ensure consistent ordering of files\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of image-mask pairs in the dataset.\"\"\"\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves and processes an image-mask pair by index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image-mask pair to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, mask) where image is a transformed RGB tensor and\n",
    "                  mask is a long tensor with class labels\n",
    "        \"\"\"\n",
    "        img_name = self.image_filenames[idx]\n",
    "        \n",
    "        # Construct full paths for data loading\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        \n",
    "        # Load and preprocess input data\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))  # Ensure RGB format\n",
    "        mask = np.array(Image.open(mask_path))                 # Load as-is for classes\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        # Convert mask to appropriate tensor type for loss computation\n",
    "        mask = mask.long()\n",
    "            \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3f815",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation Pipeline\n",
    "\n",
    "The data augmentation strategy includes:\n",
    "\n",
    "1. Image Sizing:\n",
    "   - Height: 480 pixels\n",
    "   - Width: 640 pixels\n",
    "   - Consistent aspect ratio\n",
    "\n",
    "2. Training Augmentations:\n",
    "   - Resize to target dimensions\n",
    "   - Horizontal flipping (50% probability)\n",
    "   - ImageNet normalization\n",
    "\n",
    "3. Validation Pipeline:\n",
    "   - Resize only\n",
    "   - Normalization\n",
    "   - No random augmentations\n",
    "\n",
    "The transforms ensure that:\n",
    "- Input sizes match model requirements\n",
    "- Data distribution matches pretrained expectations\n",
    "- Augmentations preserve semantic validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c018813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3989\n",
      "Number of validation samples: 203\n",
      "Number of test samples: 204\n",
      "\n",
      "Sample 0 from training set:\n",
      "Image shape: torch.Size([3, 480, 640]), Image dtype: torch.float32\n",
      "Mask shape: torch.Size([480, 640]), Mask dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Define input image dimensions\n",
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "\n",
    "# ImageNet normalization parameters for pretrained models\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, p=1.0),    # Consistent sizing\n",
    "    A.HorizontalFlip(p=0.5),                                # Random horizontal flips\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD,       # ImageNet normalization\n",
    "               max_pixel_value=255.0, p=1.0),\n",
    "    ToTensorV2(),                                           # Convert to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, p=1.0),    # Consistent sizing\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD,       # ImageNet normalization\n",
    "               max_pixel_value=255.0, p=1.0),\n",
    "    ToTensorV2(),                                           # Convert to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Configure data directories\n",
    "BASE_PROCESSED_DIR = Path(\"../data/processed/segmentation\")\n",
    "TRAIN_IMG_DIR = BASE_PROCESSED_DIR / \"train\" / \"images\"\n",
    "TRAIN_MASK_DIR = BASE_PROCESSED_DIR / \"train\" / \"masks\"\n",
    "VAL_IMG_DIR = BASE_PROCESSED_DIR / \"val\" / \"images\"\n",
    "VAL_MASK_DIR = BASE_PROCESSED_DIR / \"val\" / \"masks\"\n",
    "TEST_IMG_DIR = BASE_PROCESSED_DIR / \"test\" / \"images\"\n",
    "TEST_MASK_DIR = BASE_PROCESSED_DIR / \"test\" / \"masks\"\n",
    "\n",
    "# Initialize datasets with appropriate transforms\n",
    "train_dataset = PeatlandDataset(\n",
    "    images_dir=TRAIN_IMG_DIR, \n",
    "    masks_dir=TRAIN_MASK_DIR, \n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = PeatlandDataset(\n",
    "    images_dir=VAL_IMG_DIR, \n",
    "    masks_dir=VAL_MASK_DIR, \n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = PeatlandDataset(\n",
    "    images_dir=TEST_IMG_DIR, \n",
    "    masks_dir=TEST_MASK_DIR, \n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Verify dataset initialization\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Verify data format and types\n",
    "image, mask = train_dataset[0]\n",
    "print(f\"\\nSample 0 from training set:\")\n",
    "print(f\"Image shape: {image.shape}, Image dtype: {image.dtype}\")\n",
    "print(f\"Mask shape: {mask.shape}, Mask dtype: {mask.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea821735",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "The training pipeline is configured with:\n",
    "\n",
    "1. Learning Parameters:\n",
    "   - Learning rate: 1e-4 (AdamW optimizer)\n",
    "   - Batch size: 4 (GPU memory optimized)\n",
    "   - Training epochs: 10 (with early stopping)\n",
    "\n",
    "2. Hardware Setup:\n",
    "   - Automatic device selection\n",
    "   - GPU acceleration when available\n",
    "   - Memory-efficient processing\n",
    "\n",
    "3. Run Management:\n",
    "   - Timestamped run identification\n",
    "   - Organized metrics storage\n",
    "   - Automatic checkpoint saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n",
      "Metrics for this run will be saved in: metrics/2025-08-03_15-56-59\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 1e-4      # Initial learning rate for optimizer\n",
    "BATCH_SIZE = 4           # Batch size balanced for memory and speed\n",
    "NUM_EPOCHS = 10          # Maximum number of training epochs\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \\\n",
    "         \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Run identification and logging setup\n",
    "run_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "metrics_dir = Path(\"metrics\") / run_name\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Metrics for this run will be saved in: {metrics_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88055de9",
   "metadata": {},
   "source": [
    "## 5. DataLoader Configuration\n",
    "\n",
    "Initialize data loaders with:\n",
    "\n",
    "1. Training Settings:\n",
    "   - Shuffled batch ordering\n",
    "   - Specified batch size\n",
    "   - Memory pinning for GPU\n",
    "\n",
    "2. Validation Settings:\n",
    "   - Sequential batch ordering\n",
    "   - Consistent batch size\n",
    "   - Memory optimization\n",
    "\n",
    "The configuration ensures:\n",
    "- Efficient data loading\n",
    "- Proper randomization\n",
    "- Optimal memory usage\n",
    "- Hardware-specific optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training data loader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,              # Shuffle data each epoch\n",
    "    num_workers=0,            # Single process data loading\n",
    "    pin_memory=(DEVICE == \"cuda\")  # Pin memory for faster GPU transfer\n",
    ")\n",
    "\n",
    "# Initialize validation data loader\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,             # Sequential processing for validation\n",
    "    num_workers=0,            # Single process data loading\n",
    "    pin_memory=(DEVICE == \"cuda\")  # Pin memory for faster GPU transfer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e06df",
   "metadata": {},
   "source": [
    "## 6. Model Architecture and Training Components\n",
    "\n",
    "The segmentation pipeline consists of:\n",
    "\n",
    "1. U-Net Architecture:\n",
    "   - ResNet34 encoder backbone\n",
    "   - Pretrained ImageNet weights\n",
    "   - Skip connections for detail preservation\n",
    "   - 5-class output head\n",
    "\n",
    "2. Loss Function:\n",
    "   - Cross-Entropy Loss\n",
    "   - Multi-class classification\n",
    "   - Pixel-wise error computation\n",
    "\n",
    "3. Optimization:\n",
    "   - AdamW optimizer\n",
    "   - Weight decay regularization\n",
    "   - Learning rate configuration\n",
    "   - Gradient-based updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize U-Net model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # Encoder architecture\n",
    "    encoder_weights=\"imagenet\",     # Pretrained weights\n",
    "    in_channels=3,                  # RGB input\n",
    "    classes=5,                      # Number of segmentation classes\n",
    ").to(DEVICE)                        # Move model to appropriate device\n",
    "\n",
    "# Initialize loss function for multi-class segmentation\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Configure optimizer with weight decay for regularization\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,              # Learning rate from configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b51c36",
   "metadata": {},
   "source": [
    "## 7. Training Loop Implementation\n",
    "\n",
    "The training process includes:\n",
    "\n",
    "1. Epoch-level Processing:\n",
    "   - Training phase with gradient updates\n",
    "   - Validation phase with metric computation\n",
    "   - Progress tracking and logging\n",
    "\n",
    "2. Batch Processing:\n",
    "   - Forward pass through model\n",
    "   - Loss computation\n",
    "   - Backpropagation\n",
    "   - Optimizer updates\n",
    "\n",
    "3. Validation:\n",
    "   - No gradient computation\n",
    "   - Loss calculation\n",
    "   - Accuracy measurement\n",
    "   - Memory-efficient processing\n",
    "\n",
    "4. Metrics and Logging:\n",
    "   - Loss tracking for both phases\n",
    "   - Pixel-wise accuracy computation\n",
    "   - Progress visualization\n",
    "   - Checkpoint saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:48<00:00,  1.89it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.7886\n",
      "Average Validation Loss: 0.4593\n",
      "Validation Pixel Accuracy: 85.18%\n",
      "\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:42<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.5364\n",
      "Average Validation Loss: 0.3958\n",
      "Validation Pixel Accuracy: 85.52%\n",
      "\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:37<00:00,  1.93it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.4546\n",
      "Average Validation Loss: 0.3434\n",
      "Validation Pixel Accuracy: 88.75%\n",
      "\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:35<00:00,  1.94it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3965\n",
      "Average Validation Loss: 0.3538\n",
      "Validation Pixel Accuracy: 87.22%\n",
      "\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:36<00:00,  1.93it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3617\n",
      "Average Validation Loss: 0.3149\n",
      "Validation Pixel Accuracy: 88.82%\n",
      "\n",
      "--- Epoch 6/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:38<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3294\n",
      "Average Validation Loss: 0.3362\n",
      "Validation Pixel Accuracy: 87.87%\n",
      "\n",
      "--- Epoch 7/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:42<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3146\n",
      "Average Validation Loss: 0.3269\n",
      "Validation Pixel Accuracy: 87.88%\n",
      "\n",
      "--- Epoch 8/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:38<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2961\n",
      "Average Validation Loss: 0.3587\n",
      "Validation Pixel Accuracy: 86.58%\n",
      "\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:41<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2860\n",
      "Average Validation Loss: 0.2834\n",
      "Validation Pixel Accuracy: 89.77%\n",
      "\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 998/998 [08:38<00:00,  1.93it/s]\n",
      "Validating: 100%|██████████| 51/51 [00:17<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2642\n",
      "Average Validation Loss: 0.2856\n",
      "Validation Pixel Accuracy: 89.27%\n",
      "\n",
      "Training complete. Model saved to metrics/2025-08-03_15-56-59/peatland_segmentation_model.pth\n",
      "Training log saved to metrics/2025-08-03_15-56-59/training_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize training metrics log\n",
    "training_log = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move data to appropriate device\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        \n",
    "        # Forward pass and loss computation\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct_pixels, val_total_pixels = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Validating\"):\n",
    "            # Move data to appropriate device\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            \n",
    "            # Forward pass and loss computation\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Accumulate validation metrics\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct_pixels += (preds == masks).sum().item()\n",
    "            val_total_pixels += torch.numel(masks)\n",
    "            \n",
    "    # Calculate epoch statistics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = (val_correct_pixels / val_total_pixels) * 100\n",
    "    \n",
    "    # Display epoch results\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Pixel Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Log training metrics\n",
    "    training_log.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    })\n",
    "\n",
    "# Save final model and training metrics\n",
    "model_save_path = metrics_dir / \"peatland_segmentation_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"\\nTraining complete. Model saved to {model_save_path}\")\n",
    "\n",
    "# Save training history\n",
    "log_df = pd.DataFrame(training_log)\n",
    "log_df.to_csv(metrics_dir / \"training_log.csv\", index=False)\n",
    "print(f\"Training log saved to {metrics_dir / 'training_log.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
