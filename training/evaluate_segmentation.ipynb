{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af14e1a7",
   "metadata": {},
   "source": [
    "## 1. Required Libraries\n",
    "\n",
    "Import necessary packages for:\n",
    "- Deep learning (PyTorch)\n",
    "- Image processing\n",
    "- Metrics computation\n",
    "- Visualization\n",
    "- Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bae15",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Model Evaluation\n",
    "\n",
    "This notebook performs comprehensive evaluation of the trained semantic segmentation model for peatland terrain classification. It provides detailed metrics and visualizations to assess model performance.\n",
    "\n",
    "Key Components:\n",
    "1. Quantitative Evaluation:\n",
    "   - Pixel-wise accuracy\n",
    "   - Per-class IoU (Intersection over Union)\n",
    "   - Mean IoU across classes\n",
    "   \n",
    "2. Qualitative Analysis:\n",
    "   - Side-by-side visualization of predictions\n",
    "   - Color-coded class representation\n",
    "   - Sample-wise inspection\n",
    "\n",
    "The evaluation uses the held-out test set to ensure unbiased assessment of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stahlma/.pyenv/versions/peatland_navigation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep learning and numerical computing\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "# Image processing and data handling\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcffe3",
   "metadata": {},
   "source": [
    "## 2. Dataset Implementation\n",
    "\n",
    "The evaluation dataset provides:\n",
    "\n",
    "1. Data Loading:\n",
    "   - Test set images and masks\n",
    "   - Proper format conversion\n",
    "   - Memory-efficient processing\n",
    "\n",
    "2. Image Processing:\n",
    "   - RGB image handling\n",
    "   - Label mask loading\n",
    "   - Transformation application\n",
    "\n",
    "3. Evaluation Features:\n",
    "   - Consistent ordering\n",
    "   - Batch processing support\n",
    "   - Ground truth alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeatlandDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for evaluating peatland segmentation models.\n",
    "    \n",
    "    This dataset handles the loading and processing of test images and their\n",
    "    corresponding segmentation masks for model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str or Path): Directory containing input RGB images\n",
    "        masks_dir (str or Path): Directory containing segmentation masks\n",
    "        transform (callable, optional): Transformations to apply to image-mask pairs\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        # Initialize paths and transforms\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Ensure consistent file ordering\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of test samples.\"\"\"\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves a test image-mask pair by index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image-mask pair to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, mask) where image is a transformed RGB tensor and\n",
    "                  mask is a long tensor with class labels\n",
    "        \"\"\"\n",
    "        # Get file paths\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        # Ensure proper tensor type for evaluation\n",
    "        mask = mask.long()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de344",
   "metadata": {},
   "source": [
    "## 3. Evaluation Transforms\n",
    "\n",
    "Configure image transformations for consistent evaluation:\n",
    "\n",
    "1. Image Sizing:\n",
    "   - Fixed dimensions: 480x640\n",
    "   - Consistent aspect ratio\n",
    "   - No random augmentations\n",
    "\n",
    "2. Normalization:\n",
    "   - ImageNet mean and std values\n",
    "   - Scale to [0,1] range\n",
    "   - Channel-wise processing\n",
    "\n",
    "3. Tensor Conversion:\n",
    "   - PyTorch format\n",
    "   - Proper channel ordering\n",
    "   - Memory layout optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image dimensions for evaluation\n",
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "\n",
    "# ImageNet normalization parameters\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Evaluation transform pipeline\n",
    "eval_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),           # Consistent sizing\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD,       # Standardization\n",
    "               max_pixel_value=255.0),\n",
    "    ToTensorV2(),                                          # Convert to PyTorch tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97766aa1",
   "metadata": {},
   "source": [
    "## 4. Evaluation Configuration\n",
    "\n",
    "Setup evaluation parameters:\n",
    "\n",
    "1. Model Selection:\n",
    "   - Specific training run\n",
    "   - Model checkpoint path\n",
    "   - Batch processing settings\n",
    "\n",
    "2. Hardware Configuration:\n",
    "   - Automatic device selection\n",
    "   - GPU acceleration when available\n",
    "   - Memory optimization\n",
    "\n",
    "3. Path Management:\n",
    "   - Metrics directory structure\n",
    "   - Results organization\n",
    "   - Output path handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from run: 2025-08-03_15-56-59\n",
      "Model path: metrics/2025-08-03_15-56-59/peatland_segmentation_model.pth\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Model identification and paths\n",
    "RUN_NAME = \"2025-08-03_15-56-59\"  # Timestamp of the training run to evaluate\n",
    "METRICS_DIR = Path(\"./metrics\") / RUN_NAME\n",
    "MODEL_PATH = METRICS_DIR / \"peatland_segmentation_model.pth\"\n",
    "\n",
    "# Evaluation parameters\n",
    "BATCH_SIZE = 4  # Batch size for efficient processing\n",
    "\n",
    "# Hardware configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"      # Use GPU if available\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"       # Use Apple Silicon if available\n",
    "else:\n",
    "    DEVICE = \"cpu\"       # Fall back to CPU\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Evaluating model from run: {RUN_NAME}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9666182",
   "metadata": {},
   "source": [
    "## 5. Test Dataset Preparation\n",
    "\n",
    "Initialize the test dataset:\n",
    "\n",
    "1. Data Organization:\n",
    "   - Dedicated test set directory\n",
    "   - Separate image and mask folders\n",
    "   - Structured file hierarchy\n",
    "\n",
    "2. DataLoader Configuration:\n",
    "   - Sequential processing\n",
    "   - Memory pinning for GPU\n",
    "   - Efficient batch handling\n",
    "\n",
    "3. Testing Pipeline:\n",
    "   - Transform application\n",
    "   - Batch creation\n",
    "   - Device optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426eb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 204 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "# Define test data paths\n",
    "BASE_PROCESSED_DIR = Path(\"../data/processed/segmentation\")\n",
    "TEST_IMG_DIR = BASE_PROCESSED_DIR / \"test\" / \"images\"\n",
    "TEST_MASK_DIR = BASE_PROCESSED_DIR / \"test\" / \"masks\"\n",
    "\n",
    "# Initialize test dataset with transforms\n",
    "test_dataset = PeatlandDataset(\n",
    "    images_dir=TEST_IMG_DIR,\n",
    "    masks_dir=TEST_MASK_DIR,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "# Create test dataloader\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,             # Sequential processing for testing\n",
    "    num_workers=0,            # Single process data loading\n",
    "    pin_memory=(DEVICE == \"cuda\")  # Pin memory for faster GPU transfer\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(test_dataset)} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8241d",
   "metadata": {},
   "source": [
    "## 6. Model Initialization\n",
    "\n",
    "Load the trained model:\n",
    "\n",
    "1. Architecture Setup:\n",
    "   - U-Net with ResNet34 backbone\n",
    "   - Multi-class segmentation head\n",
    "   - No pretraining weights\n",
    "\n",
    "2. State Loading:\n",
    "   - Checkpoint restoration\n",
    "   - Device placement\n",
    "   - Evaluation mode\n",
    "\n",
    "3. Model Configuration:\n",
    "   - RGB input channels\n",
    "   - Five-class output\n",
    "   - Memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model architecture\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",     # Encoder backbone\n",
    "    encoder_weights=None,        # No pretraining for evaluation\n",
    "    in_channels=3,              # RGB input\n",
    "    classes=5,                  # Number of segmentation classes\n",
    ").to(DEVICE)                    # Move to appropriate device\n",
    "\n",
    "# Load trained model weights\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deae95",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Execute comprehensive evaluation:\n",
    "\n",
    "1. Metric Computation:\n",
    "   - Per-class IoU\n",
    "   - Mean IoU (mIoU)\n",
    "   - Pixel-wise accuracy\n",
    "\n",
    "2. Class Handling:\n",
    "   - Five-class segmentation\n",
    "   - Balanced evaluation\n",
    "   - Ignore class consideration\n",
    "\n",
    "3. Batch Processing:\n",
    "   - Memory-efficient inference\n",
    "   - Accumulated metrics\n",
    "   - Progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04300b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████| 51/51 [00:24<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Pixel Accuracy: 88.13%\n",
      "Mean IoU (mIoU): 0.7111\n",
      "\n",
      "IoU per Class:\n",
      "  - PATH: 0.6828\n",
      "  - NATURAL_GROUND: 0.7905\n",
      "  - TREE: 0.4184\n",
      "  - VEGETATION: 0.7965\n",
      "  - IGNORE: 0.8675\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation parameters\n",
    "NUM_CLASSES = 5\n",
    "CLASS_NAMES = [\"PATH\", \"NATURAL_GROUND\", \"TREE\", \"VEGETATION\", \"IGNORE\"]\n",
    "\n",
    "# Initialize metric calculators\n",
    "jaccard = JaccardIndex(\n",
    "    task=\"multiclass\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    average=None\n",
    ").to(DEVICE)\n",
    "\n",
    "# Initialize accuracy tracking\n",
    "total_correct_pixels = 0\n",
    "total_pixels = 0\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        # Move data to device\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        \n",
    "        # Generate predictions\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Update metrics\n",
    "        jaccard.update(preds, masks)\n",
    "        total_correct_pixels += (preds == masks).sum().item()\n",
    "        total_pixels += torch.numel(masks)\n",
    "\n",
    "# Calculate final metrics\n",
    "pixel_accuracy = (total_correct_pixels / total_pixels) * 100\n",
    "iou_per_class = jaccard.compute()\n",
    "mean_iou = iou_per_class.mean()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Overall Pixel Accuracy: {pixel_accuracy:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {mean_iou:.4f}\")\n",
    "print(\"\\nIoU per Class:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  - {class_name}: {iou_per_class[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22583c",
   "metadata": {},
   "source": [
    "## 8. Metrics Storage\n",
    "\n",
    "Save evaluation results:\n",
    "\n",
    "1. Metric Organization:\n",
    "   - Overall accuracy\n",
    "   - Mean IoU\n",
    "   - Per-class performance\n",
    "\n",
    "2. Data Formatting:\n",
    "   - Structured DataFrame\n",
    "   - Clear metric naming\n",
    "   - Consistent value types\n",
    "\n",
    "3. File Management:\n",
    "   - CSV format storage\n",
    "   - Run-specific directory\n",
    "   - Organized hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics saved to: metrics/2025-08-03_15-56-59/test_set_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare metrics data structure\n",
    "metrics_data = {\n",
    "    'Metric': ['Pixel Accuracy', 'Mean IoU'] + [f'IoU_{name}' for name in CLASS_NAMES],\n",
    "    'Value': [pixel_accuracy, mean_iou.item()] + iou_per_class.cpu().numpy().tolist()\n",
    "}\n",
    "\n",
    "# Create organized DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Save metrics to CSV\n",
    "output_csv_path = METRICS_DIR / \"test_set_evaluation.csv\"\n",
    "metrics_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nEvaluation metrics saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9bb16",
   "metadata": {},
   "source": [
    "## 9. Results Visualization\n",
    "\n",
    "Generate qualitative evaluation visualizations:\n",
    "\n",
    "1. Sample Selection:\n",
    "   - Random test set images\n",
    "   - Multi-sample comparison\n",
    "   - Representative coverage\n",
    "\n",
    "2. Visualization Components:\n",
    "   - Original RGB image\n",
    "   - Ground truth segmentation\n",
    "   - Model predictions\n",
    "\n",
    "3. Output Generation:\n",
    "   - Color-coded classes\n",
    "   - Side-by-side comparison\n",
    "   - High-resolution figures\n",
    "\n",
    "4. Color Scheme:\n",
    "   - Path: Purple\n",
    "   - Natural Ground: Light Purple\n",
    "   - Tree: Light Blue\n",
    "   - Vegetation: Yellow\n",
    "   - Ignore: Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ecd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 visualization samples to: metrics/2025-08-03_15-56-59/visualizations\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset, model, device, num_samples=5):\n",
    "    \"\"\"Generates and saves visualization of model predictions.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The test dataset containing image-mask pairs\n",
    "        model: The trained segmentation model\n",
    "        device: Device to run inference on\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    # Create visualization directory\n",
    "    vis_dir = METRICS_DIR / \"visualizations\"\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Define class color map (BGR format for OpenCV)\n",
    "    color_map = np.array([\n",
    "        [60, 16, 152],     # Path (Purple)\n",
    "        [132, 41, 246],    # Natural Ground (Light Purple)\n",
    "        [110, 193, 228],   # Tree (Light Blue)\n",
    "        [254, 221, 58],    # Vegetation (Yellow)\n",
    "        [0, 0, 0]          # Ignore (Black)\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    # Generate predictions and visualizations\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Load and process sample\n",
    "            image_tensor, gt_mask = dataset[i]\n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate prediction\n",
    "            output = model(input_tensor)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Prepare visualization data\n",
    "            display_image = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            display_image = (display_image * IMAGENET_STD) + IMAGENET_MEAN\n",
    "            display_image = np.clip(display_image, 0, 1)\n",
    "            \n",
    "            # Apply color mapping\n",
    "            gt_mask_color = color_map[gt_mask.cpu().numpy()]\n",
    "            pred_mask_color = color_map[pred_mask]\n",
    "\n",
    "            # Create comparison plot\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "            fig.suptitle(f\"Sample {i}\", fontsize=16)\n",
    "            \n",
    "            # Original image\n",
    "            ax[0].imshow(display_image)\n",
    "            ax[0].set_title(\"Original Image\")\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            # Ground truth\n",
    "            ax[1].imshow(gt_mask_color)\n",
    "            ax[1].set_title(\"Ground Truth Mask\")\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            # Model prediction\n",
    "            ax[2].imshow(pred_mask_color)\n",
    "            ax[2].set_title(\"Model Prediction\")\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            # Save visualization\n",
    "            plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Saved {num_samples} visualization samples to: {vis_dir}\")\n",
    "\n",
    "# Generate visualizations\n",
    "visualize_predictions(test_dataset, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe06f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
