{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af14e1a7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e6240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "from transformers import Dinov2Model, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcffe3",
   "metadata": {},
   "source": [
    "## 2. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5630cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef9c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeatlandDinoDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset adapted for DinoV2's image processor.\"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, image_processor, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=mask)\n",
    "            image = Image.fromarray(augmented['image'])\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return pixel_values, mask\n",
    "\n",
    "class DinoV2ForSemanticSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(DinoV2ForSemanticSegmentation, self).__init__()\n",
    "        self.dinov2 = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.dinov2(pixel_values, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        patch_tokens = last_hidden_state[:, 1:, :]\n",
    "        batch_size, seq_len, num_channels = patch_tokens.shape\n",
    "        height = width = int(seq_len**0.5)\n",
    "        feature_map = patch_tokens.permute(0, 2, 1).contiguous().reshape(batch_size, num_channels, height, width)\n",
    "        logits = self.head(feature_map)\n",
    "        final_logits = nn.functional.interpolate(logits, size=(DINO_IMAGE_SIZE, DINO_IMAGE_SIZE), mode='bilinear', align_corners=False)\n",
    "        return final_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de344",
   "metadata": {},
   "source": [
    "## 3. Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c7b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "eval_transform = A.Compose([A.Resize(height=DINO_IMAGE_SIZE, width=DINO_IMAGE_SIZE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97766aa1",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05b2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from run: dinov2_2025-08-03_17-48-54\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = \"dinov2_2025-08-03_17-48-54\" # <--- CHANGE THIS TO THE RUN NAME TO EVALUATE\n",
    "\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME\n",
    "# The best model was saved with this name due to early stopping\n",
    "MODEL_PATH = METRICS_DIR / \"best_model.pth\" \n",
    "BATCH_SIZE = 4\n",
    "\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "else: DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Evaluating model from run: {RUN_NAME}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9666182",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426eb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 204 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "BASE_PROCESSED_DIR = Path(\"../data/processed/segmentation\")\n",
    "TEST_IMG_DIR = BASE_PROCESSED_DIR / \"test\" / \"images\"\n",
    "TEST_MASK_DIR = BASE_PROCESSED_DIR / \"test\" / \"masks\"\n",
    "\n",
    "test_dataset = PeatlandDinoDataset(\n",
    "    images_dir=TEST_IMG_DIR,\n",
    "    masks_dir=TEST_MASK_DIR,\n",
    "    image_processor=processor,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Loaded {len(test_dataset)} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8241d",
   "metadata": {},
   "source": [
    "## 6. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b16e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = DinoV2ForSemanticSegmentation(num_classes=5).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deae95",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation and calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04300b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████| 51/51 [00:20<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Pixel Accuracy: 86.29%\n",
      "Mean IoU (mIoU): 0.6565\n",
      "\n",
      "IoU per Class:\n",
      "  - PATH: 0.7115\n",
      "  - NATURAL_GROUND: 0.7868\n",
      "  - TREE: 0.2320\n",
      "  - VEGETATION: 0.7450\n",
      "  - IGNORE: 0.8070\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "CLASS_NAMES = [\"PATH\", \"NATURAL_GROUND\", \"TREE\", \"VEGETATION\", \"IGNORE\"]\n",
    "jaccard = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(DEVICE)\n",
    "total_correct_pixels, total_pixels = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        jaccard.update(preds, masks)\n",
    "        total_correct_pixels += (preds == masks).sum().item()\n",
    "        total_pixels += torch.numel(masks)\n",
    "\n",
    "pixel_accuracy = (total_correct_pixels / total_pixels) * 100\n",
    "iou_per_class = jaccard.compute()\n",
    "mean_iou = iou_per_class.mean()\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Overall Pixel Accuracy: {pixel_accuracy:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {mean_iou:.4f}\")\n",
    "print(\"\\nIoU per Class:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  - {class_name}: {iou_per_class[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22583c",
   "metadata": {},
   "source": [
    "## 8. Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b798271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics saved to: metrics/dinov2_2025-08-03_17-48-54/test_set_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "    'Metric': ['Pixel Accuracy', 'Mean IoU'] + [f'IoU_{name}' for name in CLASS_NAMES],\n",
    "    'Value': [pixel_accuracy, mean_iou.item()] + iou_per_class.cpu().numpy().tolist()\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "output_csv_path = METRICS_DIR / \"test_set_evaluation.csv\"\n",
    "metrics_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nEvaluation metrics saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9bb16",
   "metadata": {},
   "source": [
    "## 9. Visualize Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54ecd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 visualization samples to: metrics/dinov2_2025-08-03_17-48-54/visualizations\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset, model, device, num_samples=5):\n",
    "    vis_dir = METRICS_DIR / \"visualizations\"\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    color_map = np.array([\n",
    "        [60, 16, 152], [132, 41, 246], [110, 193, 228],\n",
    "        [254, 221, 58], [0, 0, 0]\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get processed data for the model\n",
    "            image_tensor, gt_mask = dataset[i]\n",
    "            \n",
    "            # Load the original, unprocessed image for clean visualization\n",
    "            original_image_path = dataset.images_dir / dataset.image_filenames[i]\n",
    "            display_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Get prediction\n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Apply color map to masks\n",
    "            gt_mask_color = color_map[gt_mask.cpu().numpy()]\n",
    "            pred_mask_color = color_map[pred_mask]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "            fig.suptitle(f\"Sample {i}\", fontsize=16)\n",
    "            \n",
    "            ax[0].imshow(display_image)\n",
    "            ax[0].set_title(\"Original Image\")\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(gt_mask_color)\n",
    "            ax[1].set_title(\"Ground Truth Mask\")\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            ax[2].imshow(pred_mask_color)\n",
    "            ax[2].set_title(\"Model Prediction\")\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Saved {num_samples} visualization samples to: {vis_dir}\")\n",
    "\n",
    "visualize_predictions(test_dataset, model, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe06f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
