{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40221e1c",
   "metadata": {},
   "source": [
    "# Binary Segmentation Model Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook implements an evaluation framework for the binary segmentation model developed for peatland path detection. The evaluation provides critical insights into model performance and reliability.\n",
    "\n",
    "## Evaluation Components\n",
    "\n",
    "### Quantitative Metrics\n",
    "- **Pixel Accuracy**: Overall correctness of pixel classification\n",
    "- **IoU (Intersection over Union)**: Region-based accuracy assessment\n",
    "- **Class-wise Performance**: Separate metrics for path and background\n",
    "\n",
    "### Qualitative Analysis\n",
    "- **Visual Comparisons**: Side-by-side visualization of predictions\n",
    "- **Error Analysis**: Identification of common failure modes\n",
    "- **Edge Case Examination**: Assessment of challenging scenarios\n",
    "\n",
    "### Output Organization\n",
    "- Structured metric logging\n",
    "- Visualization generation\n",
    "- Statistical summaries\n",
    "- Performance reports\n",
    "\n",
    "## Significance\n",
    "The binary segmentation model forms a crucial component in identifying traversable paths in peatland environments, making its accurate evaluation essential for deployment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29c86ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stahlma/.pyenv/versions/peatland_navigation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# !pip install torchmetrics\n",
    "from torchmetrics import JaccardIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0678e",
   "metadata": {},
   "source": [
    "## 2. Dataset and Transformations\n",
    "\n",
    "The dataset implementation includes:\n",
    "\n",
    "- **BinaryPeatlandDataset**: A custom PyTorch dataset class for binary segmentation\n",
    "  - Loads image and mask pairs\n",
    "  - Handles RGB images and binary masks\n",
    "  - Supports data augmentation transformations\n",
    "\n",
    "- **Evaluation Transforms**:\n",
    "  - Resizing to consistent dimensions (480x640)\n",
    "  - Normalization using ImageNet statistics\n",
    "  - Conversion to PyTorch tensors\n",
    "\n",
    "The transforms ensure consistent input formatting while preserving the original image characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e0db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryPeatlandDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading images and BINARY (Path/BG) masks.\"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        mask_path = self.masks_dir / img_name\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        mask = mask.long()\n",
    "        return image, mask\n",
    "\n",
    "# --- Define Transforms for Evaluation ---\n",
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "eval_transform = A.Compose([\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb26161",
   "metadata": {},
   "source": [
    "## 3. Evaluation Configuration\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "1. **Run Identification**\n",
    "   - **RUN_NAME**: Unique identifier for specific model version\n",
    "   - **METRICS_DIR**: Structured storage for evaluation results\n",
    "   - **MODEL_PATH**: Location of trained model weights\n",
    "\n",
    "2. **Hardware Settings**\n",
    "   - **DEVICE**: Computation device selection (CUDA/MPS/CPU)\n",
    "   - Dynamic selection based on available hardware\n",
    "   - Optimal resource utilization\n",
    "\n",
    "3. **Batch Processing**\n",
    "   - **BATCH_SIZE**: Sample processing efficiency\n",
    "   - Memory optimization\n",
    "   - Evaluation speed consideration\n",
    "\n",
    "### Configuration Significance\n",
    "\n",
    "- **Reproducibility**: Consistent evaluation environment\n",
    "- **Resource Management**: Efficient hardware utilization\n",
    "- **Result Organization**: Structured output storage\n",
    "- **Performance Optimization**: Balanced processing speed\n",
    "\n",
    "### Implementation Notes\n",
    "The configuration ensures:\n",
    "- Reproducible evaluation runs\n",
    "- Efficient resource utilization\n",
    "- Clear result organization\n",
    "- Scalable processing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from run: binary_unet_2025-08-03_22-53-32\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Identifier for the specific training run to evaluate\n",
    "RUN_NAME = \"binary_unet_2025-08-03_22-53-32\"  # Update this for each evaluation\n",
    "\n",
    "# --- Paths and settings derived automatically ---\n",
    "METRICS_DIR = Path(\"metrics\") / RUN_NAME        # Directory for evaluation results\n",
    "MODEL_PATH = METRICS_DIR / \"best_binary_model.pth\"  # Path to model weights\n",
    "BATCH_SIZE = 4                                  # Number of samples per batch\n",
    "\n",
    "# Select appropriate device for computation\n",
    "if torch.cuda.is_available(): DEVICE = \"cuda\"       # Use GPU if available\n",
    "elif torch.backends.mps.is_available(): DEVICE = \"mps\"  # Use Apple Silicon if available\n",
    "else: DEVICE = \"cpu\"                               # Fall back to CPU\n",
    "\n",
    "# Display configuration information\n",
    "print(f\"Evaluating model from run: {RUN_NAME}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ae4c2",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc6eb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 203 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "BASE_PROCESSED_DIR = Path(\"../data/processed/binary_segmentation\")\n",
    "VAL_IMG_DIR = BASE_PROCESSED_DIR / \"val\" / \"images\"\n",
    "VAL_MASK_DIR = BASE_PROCESSED_DIR / \"val\" / \"masks\"\n",
    "\n",
    "# Note: We'll use the validation set as our test set for this evaluation\n",
    "test_dataset = BinaryPeatlandDataset(\n",
    "    images_dir=VAL_IMG_DIR,\n",
    "    masks_dir=VAL_MASK_DIR,\n",
    "    transform=eval_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Loaded {len(test_dataset)} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595ba51",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74008b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(\"resnet34\", encoder_weights=None, in_channels=3, classes=2).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ca2bd",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics and Analysis\n",
    "\n",
    "### Primary Metrics\n",
    "\n",
    "1. **Pixel Accuracy**\n",
    "   - Ratio of correctly classified pixels to total pixels\n",
    "   - Global assessment of model performance\n",
    "   - Simple but potentially misleading for imbalanced classes\n",
    "   - Baseline metric for quick assessment\n",
    "\n",
    "2. **Intersection over Union (IoU)**\n",
    "   - Class-specific overlap measurement\n",
    "   - Accounts for both false positives and negatives\n",
    "   - More stringent than pixel accuracy\n",
    "   - Key metrics computed:\n",
    "     * Background IoU: Accuracy of non-path regions\n",
    "     * Path IoU: Accuracy of path detection\n",
    "     * Mean IoU (mIoU): Overall segmentation quality\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "- **High Pixel Accuracy, Low IoU**: Potential class imbalance issues\n",
    "- **Low Path IoU**: Difficulty in precise path boundary detection\n",
    "- **Low Background IoU**: Over-detection of paths\n",
    "- **Balanced IoU Scores**: Well-rounded model performance\n",
    "\n",
    "### Validation Strategy\n",
    "The evaluation is performed on the validation set, which serves as a proxy test set, providing:\n",
    "- Unbiased performance estimates\n",
    "- Real-world performance indicators\n",
    "- Generalization assessment\n",
    "\n",
    "The combination of these metrics provides a comprehensive view of model performance and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75e78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|██████████| 51/51 [00:21<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Pixel Accuracy: 95.72%\n",
      "Mean IoU (mIoU): 0.8372\n",
      "\n",
      "IoU per Class:\n",
      "  - Background: 0.9519\n",
      "  - Path: 0.7226\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = [\"Background\", \"Path\"]\n",
    "jaccard = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=None).to(DEVICE)\n",
    "total_correct_pixels, total_pixels = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        jaccard.update(preds, masks)\n",
    "        total_correct_pixels += (preds == masks).sum().item()\n",
    "        total_pixels += torch.numel(masks)\n",
    "\n",
    "pixel_accuracy = (total_correct_pixels / total_pixels) * 100\n",
    "iou_per_class = jaccard.compute()\n",
    "mean_iou = iou_per_class.mean()\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Overall Pixel Accuracy: {pixel_accuracy:.2f}%\")\n",
    "print(f\"Mean IoU (mIoU): {mean_iou:.4f}\")\n",
    "print(\"\\nIoU per Class:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  - {class_name}: {iou_per_class[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40676eb6",
   "metadata": {},
   "source": [
    "## 7. Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae9d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics saved to: metrics/binary_unet_2025-08-03_22-53-32/test_set_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "    'Metric': ['Pixel Accuracy', 'Mean IoU'] + [f'IoU_{name}' for name in CLASS_NAMES],\n",
    "    'Value': [pixel_accuracy, mean_iou.item()] + iou_per_class.cpu().numpy().tolist()\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "output_csv_path = METRICS_DIR / \"test_set_evaluation.csv\"\n",
    "metrics_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nEvaluation metrics saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5c3c6",
   "metadata": {},
   "source": [
    "## 8. Visualize\n",
    "\n",
    "The visualization process creates side-by-side comparisons of:\n",
    "1. Original input image\n",
    "2. Ground truth segmentation mask\n",
    "3. Model's predicted segmentation\n",
    "\n",
    "Color coding:\n",
    "- **Black**: Background regions\n",
    "- **Purple**: Path regions\n",
    "\n",
    "These visualizations help in:\n",
    "- Qualitative assessment of model performance\n",
    "- Understanding typical failure cases\n",
    "- Identifying potential biases in predictions\n",
    "\n",
    "The results are saved as PNG files in the visualizations directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b22f026",
   "metadata": {},
   "source": [
    "import cv2 # For connected components analysis\n",
    "\n",
    "def visualize_and_analyze_predictions(dataset, model, device, num_samples_to_save=5):\n",
    "    \"\"\"\n",
    "    Visualizes a few sample predictions and analyzes the entire test set for\n",
    "    path presence and prediction stability (noisiness).\n",
    "    \"\"\"\n",
    "    vis_dir = METRICS_DIR / \"visualizations\"\n",
    "    vis_dir.mkdir(exist_ok=True)\n",
    "    color_map = np.array([[0, 0, 0], [60, 16, 152]], dtype=np.uint8) # BG: Black, Path: Purple\n",
    "\n",
    "    path_predicted_count = 0\n",
    "    total_path_components = 0\n",
    "    total_frames = len(dataset)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the entire dataset to gather stats\n",
    "        for i in tqdm(range(total_frames), desc=\"Analyzing all predictions\"):\n",
    "            image_tensor, gt_mask = dataset[i]\n",
    "            \n",
    "            # Get model prediction\n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "            # --- Analysis: Count frames with paths and component noisiness ---\n",
    "            if np.any(pred_mask == 1):\n",
    "                path_predicted_count += 1\n",
    "                # Convert to uint8 for cv2. 1 for path, 0 for background.\n",
    "                binary_pred_mask = (pred_mask == 1).astype(np.uint8)\n",
    "                # Get the number of disconnected path components\n",
    "                num_labels, _ = cv2.connectedComponents(binary_pred_mask, connectivity=8)\n",
    "                # num_labels includes the background component, so subtract 1\n",
    "                total_path_components += (num_labels - 1)\n",
    "\n",
    "            # --- Visualization: Save image comparisons for the first few samples ---\n",
    "            if i < num_samples_to_save:\n",
    "                # Un-normalize image for display\n",
    "                display_image = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "                display_image = (display_image * IMAGENET_STD) + IMAGENET_MEAN\n",
    "                display_image = np.clip(display_image, 0, 1)\n",
    "                \n",
    "                # Colorize masks\n",
    "                gt_mask_color = color_map[gt_mask.cpu().numpy()]\n",
    "                pred_mask_color = color_map[pred_mask]\n",
    "\n",
    "                # Create and save plot\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "                ax[0].imshow(display_image); ax[0].set_title(\"Original Image\"); ax[0].axis('off')\n",
    "                ax[1].imshow(gt_mask_color); ax[1].set_title(\"Ground Truth Mask\"); ax[1].axis('off')\n",
    "                ax[2].imshow(pred_mask_color); ax[2].set_title(\"Model Prediction\"); ax[2].axis('off')\n",
    "                plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "                plt.close()\n",
    "\n",
    "    # --- Print Analysis Results ---\n",
    "    print(f\"\\n--- Prediction Stability Analysis ---\")\n",
    "    print(f\"Total frames analyzed: {total_frames}\")\n",
    "    print(f\"Frames with a predicted path: {path_predicted_count} ({path_predicted_count/total_
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "            ax[0].imshow(display_image); ax[0].set_title(\"Original Image\"); ax[0].axis('off')\n",
    "            ax[1].imshow(gt_mask_color); ax[1].set_title(\"Ground Truth Mask\"); ax[1].axis('off')\n",
    "            ax[2].imshow(pred_mask_color); ax[2].set_title(\"Model Prediction\"); ax[2].axis('off')\n",
    "            plt.savefig(vis_dir / f\"sample_{i}_comparison.png\")\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Saved {num_samples} visualization samples to: {vis_dir}\")\n",
    "\n",
    "visualize_predictions(test_dataset, model, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
