{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a67d2b",
   "metadata": {},
   "source": [
    "# Object Detection with Depth Estimation\n",
    "\n",
    "This notebook implements a combined object detection and depth estimation pipeline for peatland navigation. It integrates:\n",
    "\n",
    "1. Object Detection:\n",
    "   - YOLO-based bench detection\n",
    "   - Confidence thresholding\n",
    "   - Real-time processing\n",
    "\n",
    "2. Depth Estimation:\n",
    "   - MiDaS depth model\n",
    "   - Relative distance calculation\n",
    "   - Object-specific depth analysis\n",
    "\n",
    "3. Visualization:\n",
    "   - Annotated video output\n",
    "   - Distance measurements\n",
    "   - Visual markers\n",
    "\n",
    "The pipeline provides enriched scene understanding by combining spatial and semantic information.\n",
    "\n",
    "## 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29017838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning and object detection\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# File and path handling\n",
    "from pathlib import Path\n",
    "\n",
    "# Image and video processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1d8a4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Setup the inference pipeline:\n",
    "\n",
    "1. Model Selection:\n",
    "   - Detection model path\n",
    "   - Confidence thresholds\n",
    "   - Device configuration\n",
    "\n",
    "2. Input/Output:\n",
    "   - Source video selection\n",
    "   - Output directory structure\n",
    "   - File path resolution\n",
    "\n",
    "3. Processing Parameters:\n",
    "   - Detection confidence\n",
    "   - Hardware acceleration\n",
    "   - Directory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f72dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from run: finetuned\n",
      "Processing video: data/video/splits/Clip_3_35s.mp4\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "RUN_NAME = \"finetuned\"                 # Name of the fine-tuned model run\n",
    "PROJECT_DIR = \"./training/metrics/detection\"       # Base directory for model artifacts\n",
    "\n",
    "# Input video selection\n",
    "VIDEO_FILENAME = \"Clip_3_35s.mp4\"      # Source video file name\n",
    "\n",
    "# Detection parameters\n",
    "CONFIDENCE_THRESHOLD = 0.4              # Minimum confidence for valid detections\n",
    "\n",
    "# Path configuration\n",
    "MODEL_PATH = Path(PROJECT_DIR) / RUN_NAME / \"weights/best.pt\"     # Model weights\n",
    "VIDEO_PATH = Path(\"./data/video/splits\") / VIDEO_FILENAME        # Source video\n",
    "OUTPUT_DIR = Path(PROJECT_DIR) / RUN_NAME / \"depth_predictions\"   # Output directory\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)                                   # Ensure output exists\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / f\"depth_{VIDEO_FILENAME}\"        # Output video path\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"      # Use GPU if available\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"       # Use Apple Silicon if available\n",
    "else:\n",
    "    DEVICE = \"cpu\"       # Fall back to CPU\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Loading model from run: {RUN_NAME}\")\n",
    "print(f\"Processing video: {VIDEO_PATH}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0d20f",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "Load and configure inference models:\n",
    "\n",
    "1. YOLO Detection Model:\n",
    "   - Load fine-tuned weights\n",
    "   - Configure class mapping\n",
    "   - Set inference parameters\n",
    "\n",
    "2. MiDaS Depth Model:\n",
    "   - Small model variant\n",
    "   - Optimized transforms\n",
    "   - GPU acceleration\n",
    "\n",
    "3. Model Configuration:\n",
    "   - Device placement\n",
    "   - Evaluation mode\n",
    "   - Transform pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c526a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO model loaded. 'bench' class ID is: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiDaS depth model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/stahlma/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "# Initialize YOLO detection model\n",
    "detection_model = YOLO(MODEL_PATH)\n",
    "\n",
    "# Get bench class ID from model configuration\n",
    "BENCH_CLASS_ID = [k for k, v in detection_model.names.items() if v == 'bench'][0]\n",
    "print(f\"YOLO model loaded. 'bench' class ID is: {BENCH_CLASS_ID}\")\n",
    "\n",
    "# Initialize MiDaS depth estimation model\n",
    "midas_model_type = \"MiDaS_small\"  # Balanced speed/accuracy model variant\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", midas_model_type)\n",
    "\n",
    "# Configure MiDaS model\n",
    "midas.to(DEVICE)        # Move to appropriate device\n",
    "midas.eval()           # Set to evaluation mode\n",
    "\n",
    "# Load appropriate MiDaS transforms\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = (midas_transforms.small_transform \n",
    "            if midas_model_type == \"MiDaS_small\" \n",
    "            else midas_transforms.dpt_transform)\n",
    "\n",
    "print(\"MiDaS depth model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee026b4",
   "metadata": {},
   "source": [
    "## 4. Combined Inference Pipeline\n",
    "\n",
    "Execute multi-model inference:\n",
    "\n",
    "1. Video Processing:\n",
    "   - Frame extraction\n",
    "   - Resolution handling\n",
    "   - Output video setup\n",
    "\n",
    "2. Detection Pipeline:\n",
    "   - YOLO inference\n",
    "   - Confidence filtering\n",
    "   - Bounding box extraction\n",
    "\n",
    "3. Depth Analysis:\n",
    "   - MiDaS inference\n",
    "   - Depth map generation\n",
    "   - Distance calculation\n",
    "\n",
    "4. Visualization:\n",
    "   - Bounding box drawing\n",
    "   - Distance annotation\n",
    "   - Video composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1065 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1065/1065 [02:47<00:00,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distance metric (no-GT) sanity checks ---\n",
      "Trend: Spearman(time, -distance) = 0.779  (↑ better)\n",
      "Trend: % steps decreasing        = 44.4%\n",
      "Geom consistency: Spearman(d, 1/sqrt(area)) = 0.089  (↑ better)\n",
      "Geom consistency: CV of K=d*sqrt(area)      = 0.265      (↓ better)\n",
      "Smoothness: median|Δd| / median(d)          = 0.0599 (↓ better)\n",
      "Robustness: IQR(d) = 0.124,  IQR(K) = 39.145        (↓ better)\n",
      "\n",
      "Processing complete.\n",
      "Output video with depth estimation saved to: training/metrics/detection/finetuned/depth_predictions/depth_Clip_3_35s.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {VIDEO_PATH}\")\n",
    "else:\n",
    "    # Configure video parameters\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))     # Video width\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))   # Video height\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))                     # Frame rate\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))    # Total frames\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')                 # Video codec\n",
    "    out = cv2.VideoWriter(str(OUTPUT_VIDEO_PATH), fourcc, fps, \n",
    "                         (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing {total_frames} frames...\")\n",
    "\n",
    "    dist_series = []\n",
    "    area_series = []\n",
    "    frame_idx = []\n",
    "    \n",
    "    # Process each frame\n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # 1. Object Detection\n",
    "        results = detection_model(frame, verbose=False)\n",
    "        annotated_frame = frame.copy()  # Create fresh copy for annotations\n",
    "        \n",
    "        # 2. Depth Estimation\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        input_batch = transform(img_rgb).to(DEVICE)       # Prepare for MiDaS\n",
    "        \n",
    "        # Generate depth map\n",
    "        with torch.no_grad():\n",
    "            prediction = midas(input_batch)\n",
    "            # Resize depth map to match frame\n",
    "            depth_map = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img_rgb.shape[:2],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze().cpu().numpy()\n",
    "\n",
    "        # 3. Combine Detection and Depth\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                # Filter detections by confidence and class\n",
    "                if box.cls == BENCH_CLASS_ID and box.conf >= CONFIDENCE_THRESHOLD:\n",
    "                    # Extract bounding box coordinates\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "                    w = max(1, x2 - x1)\n",
    "                    h = max(1, y2 - y1)\n",
    "                    area = float(w * h)\n",
    "\n",
    "                    dist_series.append(float(distance))\n",
    "                    area_series.append(area)\n",
    "                    frame_idx.append(len(frame_idx))\n",
    "                    \n",
    "                    # Calculate relative distance using depth\n",
    "                    box_depth = depth_map[y1:y2, x1:x2]           # Get depth in box\n",
    "                    median_depth_value = np.median(box_depth)      # Robust depth estimate\n",
    "                    distance = 1 / median_depth_value * 100        # Scale for readability\n",
    "                    \n",
    "                    # Draw annotations\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), \n",
    "                                (0, 255, 0), 2)                    # Box\n",
    "                    label = f\"Bench | Dist: {distance*2:.1f}\"        # Distance label\n",
    "                    cv2.putText(annotated_frame, label, (x1, y1 - 10), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Write annotated frame\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    d = np.array(dist_series, dtype=np.float64)\n",
    "    a = np.array(area_series, dtype=np.float64)\n",
    "    t = np.arange(len(d), dtype=np.float64)\n",
    "\n",
    "    def spearman_rho(x, y):\n",
    "        rx = np.argsort(np.argsort(x))\n",
    "        ry = np.argsort(np.argsort(y))\n",
    "        rx = (rx - rx.mean()) / (rx.std() + 1e-8)\n",
    "        ry = (ry - ry.mean()) / (ry.std() + 1e-8)\n",
    "        return float((rx * ry).mean())\n",
    "\n",
    "    # A) Trend\n",
    "    dd = np.diff(d)\n",
    "    pct_decreasing = 100.0 * (dd < 0).mean() if dd.size else np.nan\n",
    "    rho_time = spearman_rho(t, -d)  # expect positive if d decreases over time\n",
    "\n",
    "    # B) Pinhole scale consistency\n",
    "    inv_sqrt_area = 1.0 / np.sqrt(np.clip(a, 1.0, None))\n",
    "    rho_geom = spearman_rho(d, inv_sqrt_area)  # expect high positive\n",
    "    K = d * np.sqrt(np.clip(a, 1.0, None))\n",
    "    K_cv = float(np.std(K) / (np.mean(K) + 1e-8)) if K.size else np.nan  # lower is better\n",
    "\n",
    "    # C) Smoothness (scale-invariant)\n",
    "    smooth_idx = float(np.median(np.abs(dd)) / (np.median(d) + 1e-8)) if dd.size else np.nan  # lower is better\n",
    "\n",
    "    # D) Robustness (dispersion)\n",
    "    iqr_d = float(np.percentile(d, 75) - np.percentile(d, 25)) if d.size else np.nan\n",
    "    iqr_K = float(np.percentile(K, 75) - np.percentile(K, 25)) if K.size else np.nan\n",
    "\n",
    "    print(\"\\n--- Distance metric (no-GT) sanity checks ---\")\n",
    "    print(f\"Trend: Spearman(time, -distance) = {rho_time:.3f}  (↑ better)\")\n",
    "    print(f\"Trend: % steps decreasing        = {pct_decreasing:.1f}%\")\n",
    "    print(f\"Geom consistency: Spearman(d, 1/sqrt(area)) = {rho_geom:.3f}  (↑ better)\")\n",
    "    print(f\"Geom consistency: CV of K=d*sqrt(area)      = {K_cv:.3f}      (↓ better)\")\n",
    "    print(f\"Smoothness: median|Δd| / median(d)          = {smooth_idx:.4f} (↓ better)\")\n",
    "    print(f\"Robustness: IQR(d) = {iqr_d:.3f},  IQR(K) = {iqr_K:.3f}        (↓ better)\")   \n",
    "    \n",
    "    # Clean up resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Output video with depth estimation saved to: {OUTPUT_VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee46313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peatland_navigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
